{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIF_sprint17-rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はIMDBデータセットのネガポジ判定を問題として扱う。\n",
    "\n",
    "使用するモデルは**RNN(Recurrent Neural Network)**を使用する。\n",
    "\n",
    "RNNは時系列データを扱うために、名前の通り前の時系列の出力を入力として使用する再帰的な箇所が存在し、これによって時系列（データの順序性）の特徴を抽出することができる。\n",
    "\n",
    "このため、言語処理、音声認識、株価予測、動画データなどを扱うことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kzfm/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Embedding, SimpleRNN, LSTM, GRU\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset\n",
      "padding\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 40\n",
    "batch_size = 32\n",
    "\n",
    "print('load dataset')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "train_size = 5000\n",
    "test_size = 5000\n",
    "\n",
    "x_train, y_train, x_test, y_test = x_train[:train_size], y_train[:train_size], x_test[:test_size], y_test[:test_size]\n",
    "\n",
    "print('padding')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 40, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 32)                5120      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 1,285,152\n",
      "Trainable params: 1,285,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "train\n",
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 7s 1ms/step - loss: 0.6345 - acc: 0.6334 - val_loss: 0.5424 - val_acc: 0.7244\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.2740 - acc: 0.8986 - val_loss: 0.5908 - val_acc: 0.7102\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.0530 - acc: 0.9892 - val_loss: 0.6678 - val_acc: 0.7162\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.0099 - acc: 0.9996 - val_loss: 0.7460 - val_acc: 0.7394\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.8006 - val_acc: 0.7404\n",
      "5000/5000 [==============================] - 1s 220us/step\n",
      "Test score: 0.8005634890913963\n",
      "Test accuracy: 0.7404\n"
     ]
    }
   ],
   "source": [
    "print('build model')\n",
    "inp = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features, 128)(inp)\n",
    "simple_rnn_out = SimpleRNN(32, use_bias=False)(x)\n",
    "predictions = Dense(1, use_bias=False, activation='sigmoid')(simple_rnn_out)\n",
    "# predictions = Dense(1)(simple_rnn_out)\n",
    "model = Model(inputs=inp, outputs=predictions)\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('train')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model\n",
      "train\n",
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.5948 - acc: 0.6682 - val_loss: 0.4924 - val_acc: 0.7624\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 13s 3ms/step - loss: 0.3370 - acc: 0.8588 - val_loss: 0.4935 - val_acc: 0.7626\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.1709 - acc: 0.9380 - val_loss: 0.6112 - val_acc: 0.7496\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0961 - acc: 0.9704 - val_loss: 0.7372 - val_acc: 0.7486\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0559 - acc: 0.9810 - val_loss: 0.9505 - val_acc: 0.7396\n",
      "5000/5000 [==============================] - 2s 345us/step\n",
      "Test score: 0.9504947330951691\n",
      "Test accuracy: 0.7396\n"
     ]
    }
   ],
   "source": [
    "# keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "\n",
    "print('build model')\n",
    "inp = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features, 128)(inp)\n",
    "simple_rnn_out = LSTM(32)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(simple_rnn_out)\n",
    "model = Model(inputs=inp, outputs=predictions)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('train')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model\n",
      "train\n",
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.6122 - acc: 0.6508 - val_loss: 0.5123 - val_acc: 0.7490\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.3467 - acc: 0.8516 - val_loss: 0.5525 - val_acc: 0.7596\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.1837 - acc: 0.9310 - val_loss: 0.5964 - val_acc: 0.7560\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0907 - acc: 0.9718 - val_loss: 0.7913 - val_acc: 0.7408\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0515 - acc: 0.9844 - val_loss: 0.9379 - val_acc: 0.7508\n",
      "5000/5000 [==============================] - 2s 303us/step\n",
      "Test score: 0.9379061033248901\n",
      "Test accuracy: 0.7508\n"
     ]
    }
   ],
   "source": [
    "print('build model')\n",
    "inp = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features, 128)(inp)\n",
    "simple_rnn_out = GRU(32)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(simple_rnn_out)\n",
    "model = Model(inputs=inp, outputs=predictions)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('train')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "３モデルともあまり正答率に違いは見られなかったが、SGDは訓練データでの正答率が100%になってしまっているが、LSTMとGRUはまだ100%になっていない。\n",
    "\n",
    "### Kerasの中間層の出力データを取得\n",
    "\n",
    "今回はデータセットしてEmbedding層の出力である単語が分散ベクトル表現になったデータを使用する。\n",
    "\n",
    "そのためKerasの中間層からの出力を取り出すコードを書く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset\n",
      "padding\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 40\n",
    "batch_size = 32\n",
    "\n",
    "print('load dataset')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "train_size =10000\n",
    "test_size = 10000\n",
    "\n",
    "x_train, y_train, x_test, y_test = x_train[:train_size], y_train[:train_size], x_test[:test_size], y_test[:test_size]\n",
    "\n",
    "print('padding')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中間層から分散表現のデータを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "EMB (Embedding)              (None, 40, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 32)                5152      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,285,185\n",
      "Trainable params: 1,285,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('build model')\n",
    "inp = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features, 128, name='EMB')(inp)\n",
    "simple_rnn_out = SimpleRNN(32)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(simple_rnn_out)\n",
    "model = Model(inputs=inp, outputs=predictions)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "layer_name = 'EMB'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                  outputs=model.get_layer(layer_name).output)\n",
    "x_train_vector = intermediate_layer_model.predict(x_train)\n",
    "x_test_vector = intermediate_layer_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vector.shape\n",
    "x_train = x_train_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vector.shape\n",
    "x_test = x_test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 40, 128)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データが抽出できたのでスクラッチではこのデータを利用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chainerでの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB datasets\n",
      "read imdb\n",
      "constract vocabulary based on frequency\n",
      "# train data: 25000\n",
      "# test  data: 25000\n",
      "# vocab: 20000\n",
      "# class: 2\n",
      "<class 'type'>\n",
      "STRAT Training!\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           0.491309    0.118182              0.902344       1                         124.664       \n",
      "\u001b[J2           0.02038     0.00029076            1              1                         259.623       \n",
      "\u001b[J3           0.000178314  9.32843e-05           1              1                         385.878       \n",
      "\u001b[JFinished!\n"
     ]
    }
   ],
   "source": [
    "from chainer.datasets import tuple_dataset\n",
    "import text_datasets\n",
    "import chainer\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "import nets\n",
    "from nlp_utils import convert_seq\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    args={\n",
    "        'gpu':-1,\n",
    "        'dataset': 'imdb.binary',\n",
    "        'model': 'rnn',\n",
    "        'batchsize': 64,\n",
    "        'epoch': 3,\n",
    "        'out': 'result',\n",
    "        'unit': 100,\n",
    "        'layer':1,\n",
    "        'dropout':0.4,\n",
    "        'char_based': False\n",
    "    }\n",
    "\n",
    "    # Load a dataset\n",
    "    if args['dataset'] == 'dbpedia':\n",
    "        train, test, vocab = text_datasets.get_dbpedia(\n",
    "            char_based=args['char_based'])\n",
    "    elif args['dataset'].startswith('imdb.'):\n",
    "        print(\"IMDB datasets\")\n",
    "        train, test, vocab = text_datasets.get_imdb(\n",
    "            fine_grained=args['dataset'].endswith('.fine'),\n",
    "            char_based=args['char_based'])\n",
    "    elif args['dataset'] in ['TREC', 'stsa.binary', 'stsa.fine',\n",
    "                          'custrev', 'mpqa', 'rt-polarity', 'subj']:\n",
    "        train, test, vocab = text_datasets.get_other_text_dataset(\n",
    "            args['dataset'], char_based=args['char_based'])\n",
    "\n",
    "    print('# train data: {}'.format(len(train)))\n",
    "    print('# test  data: {}'.format(len(test)))\n",
    "    print('# vocab: {}'.format(len(vocab)))\n",
    "    n_class = len(set([int(d[1]) for d in train]))\n",
    "    print('# class: {}'.format(n_class))\n",
    "    \n",
    "\n",
    "    train_iter = chainer.iterators.SerialIterator(train[:1000], args['batchsize'])\n",
    "    test_iter = chainer.iterators.SerialIterator(test[:1000], args['batchsize'],\n",
    "                                                 repeat=False, shuffle=False)\n",
    "\n",
    "    # return train_iter, test_iter\n",
    "    # Setup a model\n",
    "    if args['model'] == 'rnn':\n",
    "        Encoder = nets.RNNEncoder\n",
    "        print(type(Encoder))\n",
    "    elif args['model'] == 'cnn':\n",
    "        Encoder = nets.CNNEncoder\n",
    "    elif args['model'] == 'bow':\n",
    "        Encoder = nets.BOWMLPEncoder\n",
    "\n",
    "    encoder = Encoder(n_layers=args['layer'], n_vocab=len(vocab),\n",
    "                      n_units=args['unit'], dropout=args['dropout'])\n",
    "    model = nets.TextClassifier(encoder, n_class)\n",
    "    if args['gpu'] >= 0:\n",
    "        # Make a specified GPU current\n",
    "        chainer.backends.cuda.get_device_from_id(args['gpu']).use()\n",
    "        model.to_gpu()  # Copy the model to the GPU\n",
    "    \n",
    "\n",
    "    # Setup an optimizer\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(1e-4))\n",
    "\n",
    "\n",
    "    # Set up a trainer\n",
    "    updater = training.updaters.StandardUpdater(\n",
    "        train_iter, optimizer,\n",
    "        converter=convert_seq, device=args['gpu'])\n",
    "    trainer = training.Trainer(updater, (args['epoch'], 'epoch'), out=args['out'])\n",
    "\n",
    "    # Evaluate the model with the test dataset for each epoch\n",
    "    trainer.extend(extensions.Evaluator(\n",
    "        test_iter, model,\n",
    "        converter=convert_seq, device=args['gpu']))\n",
    "\n",
    "    # Take a best snapshot\n",
    "    record_trigger = training.triggers.MaxValueTrigger(\n",
    "        'validation/main/accuracy', (1, 'epoch'))\n",
    "    trainer.extend(extensions.snapshot_object(\n",
    "        model, 'best_model.npz'),\n",
    "        trigger=record_trigger)\n",
    "\n",
    "    # Write a log of evaluation statistics for each epoch\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "    # Print a progress bar to stdout\n",
    "    trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "    print(\"STRAT Training!\")\n",
    "    # Run the training\n",
    "    trainer.run()\n",
    "    print(\"Finished!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スクラッチによるSimpleRNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recurrent:\n",
    "    \"\"\"\n",
    "    Vanilla RNNレイヤー\n",
    "    バイアス項なし\n",
    "    \n",
    "    コードでは下記の変数名を使用する\n",
    "    N : 文書（データ）数\n",
    "    T : 時系列（今回の場合は何番目の単語か）\n",
    "    D : 各単語のベクトルの次元数\n",
    "    H : 隠れ層のユニット数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,W, U):\n",
    "        \n",
    "        self.W, self.U = W, U\n",
    "        self.dW, self.dU = np.zeros_like(self.W), np.zeros_like(self.U)\n",
    "        self.data_size, self.len_word, self.output_dim = None, None, None\n",
    "        \n",
    "        self.x, self.out = None, None\n",
    "\n",
    "        \n",
    "    def step_forward(self, x_current, prev_state):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        x_current  : shape [N D] 文ごとの現在の順序の単語ベクトル\n",
    "        prev_state : shape [N H] 前の単語までの状態（出力）\n",
    "        \n",
    "        return:\n",
    "        current_state : shape [N H] 現在の単語までの状態（出力）　次の時系列で使用\n",
    "        add_affines : [N H] 活性化関数のbackwardで使用する\n",
    "        \"\"\"\n",
    "        # それぞれ重みを乗算して足し合わせる\n",
    "        add_affines = np.dot(x_current, self.U) + np.dot(prev_state, self.W)\n",
    "        # 活性化関数(tanh)を通す\n",
    "        current_state = np.tanh(add_affines)\n",
    "        \n",
    "        return current_state, add_affines\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x  : shape [N T D] 入力データ　（データ数　単語数　ベクトルの次元）\n",
    "        \"\"\"\n",
    "        N, T, D = x.shape\n",
    "        H = self.W.shape[0]\n",
    "        \n",
    "        # 前の単語のフォワードデータを格納する\n",
    "        self.prev_state = np.zeros([T, N,  H])\n",
    "        # tanhのbackwardで利用するため 加算レイヤーでの値を保持する\n",
    "        self.add_affines = np.zeros(([T, N,  H]))\n",
    "        \n",
    "        # 初期値　もっと良い書き方ないか \n",
    "        initial_prev_state = np.zeros([N, H])\n",
    "        \n",
    "        # 単語数分繰り返す\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                self.prev_state[t], self.add_affines[t] = self.step_forward(x[:,t,:], initial_prev_state)\n",
    "            else:\n",
    "                self.prev_state[t], self.add_affines[t] = self.step_forward(x[:,t,:], self.prev_state[t-1])\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        # 今回はmany to one なので最後の状態のみを出力とする\n",
    "        return self.prev_state[-1]\n",
    "    \n",
    "\n",
    "    def step_backward(self, t, dout=None):\n",
    "        '''\n",
    "        誤差逆伝播の１サイクル分\n",
    "        一番最新（順番が最後）からさかのぼっていく\n",
    "        dout : 後ろの層から来た誤差\n",
    "        ds : 前のサイクルから来た状態の誤差\n",
    "        '''    \n",
    "        # tanhのbackward\n",
    "        delta = (1 - np.tanh(self.add_affines[t])**2) * dout\n",
    "        \n",
    "        # W prev_s側\n",
    "        self.dW += np.dot(self.prev_state[t-1].T, delta)\n",
    "        ds  = np.dot(delta, self.W.T)\n",
    "        \n",
    "        # U x 側\n",
    "        self.dU += np.dot(self.x[:, t,:].T, delta)\n",
    "        dx = np.dot(delta, self.U.T) \n",
    "        \n",
    "        return dx, ds\n",
    "    \n",
    "    \n",
    "    def backward(self, dout=None):\n",
    "        \"\"\"\n",
    "        back propagation through time\n",
    "        \"\"\"\n",
    "\n",
    "        N, T, D = x.shape\n",
    "        H = self.W.shape[0]\n",
    "        \n",
    "        if dout is None:\n",
    "            dout = np.ones([N, H])\n",
    "        \n",
    "        dx = np.zeros_like(self.x)\n",
    "        \n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.dU = np.zeros_like(self.U)\n",
    "\n",
    "        ds =dout\n",
    "        for t in reversed(range(T)):\n",
    "#         for t in reversed(range(T-5, T)):\n",
    "            dx_once, ds = self.step_backward(t, ds)\n",
    "        \n",
    "        # XXX : 最後のxの出力が分からないので０で返す \n",
    "        # 現状は第一層なので捨てられるので問題ない\n",
    "        # 単語数分やらないのにdxを算出できないのでは？\n",
    "        return dx\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_unit_num = 32\n",
    "\n",
    "# test_U = np.random.randn(x_train[:10].shape[2], hidden_unit_num)\n",
    "# test_W = np.random.randn(hidden_unit_num, hidden_unit_num)\n",
    "\n",
    "# rnn_layer = Recurrent(test_W, test_U)\n",
    "# rnn_layer.forward(x_train[:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dx = rnn_layer.backward(rnn_layer.forward(x_train_vector[:10]))\n",
    "# dx.shape # 今の所０で返ってくる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine_mod:\n",
    "    \"\"\"\n",
    "    全結合層の修正版（バイアス項を除いただけ）\n",
    "    \"\"\"\n",
    "    def __init__(self, W):\n",
    "        self.W =W\n",
    "        #self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        #self.db = None\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.x, self.W)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        #self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class MultiLayerNet:\n",
    "    \n",
    "    \"\"\"全結合による多層ニューラルネットワーク\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size_list, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        #self.hidden_size_list = hidden_size_list\n",
    "\n",
    "        self.params = {}\n",
    "\n",
    "        # 重みの初期化\n",
    "        hidden_unit_num = 128\n",
    "        self.params['U'] = np.random.randn(self.input_size, hidden_unit_num)/np.sqrt(self.input_size)\n",
    "        self.params['W1'] = np.random.randn(hidden_unit_num, hidden_unit_num)  /np.sqrt(hidden_unit_num)\n",
    "        self.params['W2'] = np.random.randn(hidden_unit_num, 2)/ np.sqrt(hidden_unit_num)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['RNN'] = Recurrent(self.params['W1'], self.params['U'])\n",
    "        self.layers['Affine'] = Affine_mod(self.params['W2'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y_pred = self.predict(x)\n",
    "        \n",
    "        return self.last_layer.forward(y_pred, t)\n",
    "    \n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        pred = self.predict(x)\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(pred == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "            \"\"\"勾配を求める（数値微分）\n",
    "            Parameters\n",
    "            ----------\n",
    "            x : 入力データ\n",
    "            t : 教師ラベル\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            各層の勾配を持ったディクショナリ変数\n",
    "            \"\"\"\n",
    "            loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "            grads = {}\n",
    "            grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "            grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "            grads['U'] = numerical_gradient(loss_W, self.params['U'])\n",
    "\n",
    "            return grads\n",
    "        \n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['RNN'].dW\n",
    "        grads['W2'] = self.layers['Affine'].dW\n",
    "        grads['U'] = self.layers['RNN'].dU\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_onehot(y):\n",
    "    y1 = y.copy()\n",
    "    y1[y==0]=1\n",
    "    y1[y==1]=0\n",
    "    return np.array([y,y1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss： 0.8139920403510761 train_acc： 0.5065 test_loss： 0.8168852435195034 test_acc： 0.5046\n",
      "train_loss： 0.6722510805251091 train_acc： 0.5863 test_loss： 0.6792146453038588 test_acc： 0.5618\n",
      "train_loss： 0.6780843111785009 train_acc： 0.5699 test_loss： 0.6869311588152331 test_acc： 0.5391\n",
      "train_loss： 0.6713839766901157 train_acc： 0.5795 test_loss： 0.6909670273799128 test_acc： 0.5382\n",
      "train_loss： 0.6506039664298588 train_acc： 0.6116 test_loss： 0.6799258460800051 test_acc： 0.5659\n",
      "train_loss： 0.6455638532339809 train_acc： 0.6225 test_loss： 0.6772402524121597 test_acc： 0.5735\n",
      "train_loss： 0.6497818934386594 train_acc： 0.6159 test_loss： 0.6910614726001247 test_acc： 0.5565\n",
      "train_loss： 0.6366044452765692 train_acc： 0.633 test_loss： 0.6775841554086771 test_acc： 0.5758\n",
      "train_loss： 0.6467760469764591 train_acc： 0.6164 test_loss： 0.6826727444833941 test_acc： 0.569\n",
      "train_loss： 0.6188319703277056 train_acc： 0.6543 test_loss： 0.6608198261412277 test_acc： 0.6132\n"
     ]
    }
   ],
   "source": [
    "from common.optimizer import Adam, SGD\n",
    "\n",
    "t_train = convert_onehot(y_train) #onehot\n",
    "t_test = convert_onehot(y_test) #onehot\n",
    "\n",
    "network = MultiLayerNet(input_size=128, hidden_size_list=[32], output_size=2)\n",
    "optimizer=Adam(lr=0.005)\n",
    "\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "iteration = 100\n",
    "for i in range(iteration):\n",
    "\n",
    "    batch_mask_train = np.random.choice(train_size, batch_size)\n",
    "    x_batch_train = x_train[batch_mask_train]\n",
    "    t_batch_train = t_train[batch_mask_train]\n",
    "    \n",
    "    # 勾配\n",
    "    grads = network.gradient(x_batch_train, t_batch_train)\n",
    "    optimizer.update(network.params, grads)\n",
    "    \n",
    "    \n",
    "#     # 更新\n",
    "#     for key in ('W1', 'W2', 'U'):\n",
    "#         network.params[key] -= learning_rate * grads[key]\n",
    "\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        loss_train = network.loss(x_train, t_train)\n",
    "        loss_test = network.loss(x_test, t_test)\n",
    "        #train_acc_list.append(train_acc)\n",
    "        #test_acc_list.append(test_acc)\n",
    "        print(\"train_loss：\",loss_train,\"train_acc：\",train_acc,\"test_loss：\",loss_test,\"test_acc：\",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配チェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:0.010221332142711263\n",
      "W2:4.0218106116323866e-09\n",
      "U:0.002551621590229474\n"
     ]
    }
   ],
   "source": [
    "network = MultiLayerNet(input_size=128, hidden_size_list=[32], output_size=2)\n",
    "optimizer=Adam()\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "optimizer.update(network.params, grads)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### BPTTですべてループさせた場合\n",
    "\n",
    "最初の時系列（単語）まで遡って勾配爆発、消失が起きるか確認したが、\n",
    "ロスや正答率に変化はみられなかった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss： 0.6984967825351835 train_acc： 0.505 test_loss： 0.6989442399108108 test_acc： 0.4971\n",
      "train_loss： 0.6766165253785991 train_acc： 0.5699 test_loss： 0.6790924066496276 test_acc： 0.5703\n",
      "train_loss： 0.6832224798699371 train_acc： 0.5748 test_loss： 0.6895291441466357 test_acc： 0.5767\n",
      "train_loss： 0.6733013636880936 train_acc： 0.5845 test_loss： 0.6883546607438253 test_acc： 0.5347\n",
      "train_loss： 0.6479155156152586 train_acc： 0.6278 test_loss： 0.6673016237179252 test_acc： 0.5832\n",
      "train_loss： 0.644868876445985 train_acc： 0.6261 test_loss： 0.6716900961771798 test_acc： 0.5792\n",
      "train_loss： 0.6279058093227331 train_acc： 0.647 test_loss： 0.6597144767566026 test_acc： 0.6033\n",
      "train_loss： 0.6238428352705152 train_acc： 0.6498 test_loss： 0.6665348683067512 test_acc： 0.5934\n",
      "train_loss： 0.6183937558690431 train_acc： 0.6587 test_loss： 0.6616180374014161 test_acc： 0.6136\n",
      "train_loss： 0.6213556691457708 train_acc： 0.6533 test_loss： 0.6650325964289392 test_acc： 0.5974\n"
     ]
    }
   ],
   "source": [
    "from common.optimizer import Adam, SGD\n",
    "\n",
    "t_train = convert_onehot(y_train) #onehot\n",
    "t_test = convert_onehot(y_test) #onehot\n",
    "\n",
    "network = MultiLayerNet(input_size=128, hidden_size_list=[32], output_size=2)\n",
    "optimizer=Adam(lr=0.002)\n",
    "\n",
    "iters_num = 100\n",
    "train_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "\n",
    "    batch_mask_train = np.random.choice(train_size, batch_size)\n",
    "    x_batch_train = x_train[batch_mask_train]\n",
    "    t_batch_train = t_train[batch_mask_train]\n",
    "    \n",
    "    # 勾配\n",
    "    grads = network.gradient(x_batch_train, t_batch_train)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        loss_train = network.loss(x_train, t_train)\n",
    "        loss_test = network.loss(x_test, t_test)\n",
    "        print(\"train_loss：\",loss_train,\"train_acc：\",train_acc,\"test_loss：\",loss_test,\"test_acc：\",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
