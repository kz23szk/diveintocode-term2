{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNNスクラッチコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのロード\n",
    "\n",
    "mnistの手書き文字認識問題\n",
    "\n",
    "784ピクセルの値（0-255）から数字（0-9）を分類する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kzfm/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# プロトタイプなので100データだけ使用する\n",
    "x_train = x_train[:100]\n",
    "y_train = y_train[:100]\n",
    "y_label = y_train\n",
    "y_train = np.identity(10)[y_train]\n",
    "del x_test, y_test\n",
    "\n",
    "\n",
    "# 訓練とテストデータに分割\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train : (100, 32, 32, 3)\n",
      "y_train : (100, 1, 10)\n",
      "y_label : (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# カラー画像のデータセット\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train_cifar10, y_train_cifar10), (x_test_cifar10, y_test_cifar10) = cifar10.load_data()\n",
    "\n",
    "# プロトタイプなので100データだけ使用する\n",
    "x_train_cifar10 = x_train_cifar10[:100]\n",
    "y_train_cifar10 = y_train_cifar10[:100]\n",
    "y_label_cifar10 = y_train_cifar10\n",
    "y_train_cifar10 = np.identity(10)[y_train_cifar10]\n",
    "del x_test_cifar10, y_test_cifar10\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('x_train : {}'.format(x_train_cifar10.shape))\n",
    "print('y_train : {}'.format(y_train_cifar10.shape))\n",
    "print('y_label : {}'.format(y_label_cifar10.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正答率を算出する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 正答率を算出する\n",
    "# def accuracy_score(X, y, params):\n",
    "#     y_pred =  predict(X, params)\n",
    "#     y_pred_number = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "#     y_label = np.argmax(y, axis=1)\n",
    "    \n",
    "#     data_size = X.shape[0]\n",
    "    \n",
    "#     correct_count = np.sum([y_label == y_pred_number]) \n",
    "#     score = correct_count / data_size * 100\n",
    "    \n",
    "#     return round(score, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### レイヤークラス\n",
    "\n",
    "親クラスとしてlayerクラスを作成。\n",
    "\n",
    "以後、各機能のレイヤークラスはこの親クラスを継承することにする。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, params={}):\n",
    "        if 'input_size' in params:\n",
    "            self.input_shape = params['input_shape']\n",
    "        else:\n",
    "            self.input_shape = 0\n",
    "            \n",
    "        if 'output_size' in params:\n",
    "            self.output_shape = params['output_shape']\n",
    "        else:\n",
    "            self.output_shape = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最終層\n",
    "\n",
    "分類問題用の最終層クラス（ソフトマックスとクロスエントロピーの結合層）\n",
    "\n",
    "TODO: 正則化項がついていない。　すべての層の重みWを集計するメソッドをNetworkクラスに作成してそれを呼ばなくてはいけないため後回し。\n",
    "\n",
    "回帰問題用に別クラスが必要になる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソフトマックス関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コスト関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, y_pred):\n",
    "    \n",
    "    data_size = y.shape[0]\n",
    "\n",
    "    # クロスエントロピー誤差関数　y_predは０になりえるので -inf にならないためにすごく小さい補正値を入れる\n",
    "    cross_entorpy = -np.sum(y * np.log(y_pred + 1e-7))\n",
    "    \n",
    "    error = cross_entorpy  / data_size\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以前の正則化項つきのクロスエントロピー関数　いずれ使うのでそのままにしておく。\n",
    "# def cost(y, y_pred, params, lam=0.01):\n",
    "#     data_size = y.shape[0]\n",
    "#     #  正則化項\n",
    "#     weight_sum = sum([np.sum(matrix**2) for key, matrix in params.items() if \"W\" in key])\n",
    "#     reg_term = (lam /2) * (weight_sum)\n",
    "#     # クロスエントロピー誤差関数　y_predは０になりえるので -inf にならないためにすごく小さい補正値を入れる\n",
    "#     cross_entorpy = -np.sum(y * np.log(y_pred + 1e-7))\n",
    "    \n",
    "#     cost = (cross_entorpy + reg_term) / data_size\n",
    "#     return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss(Layer):\n",
    "    def __init__(self, params={}):\n",
    "        super(SoftmaxWithLoss, self).__init__(params)\n",
    "        self.loss = None # 損失関数\n",
    "        self.y = None       # softmaxの出力\n",
    "        self.t = None       # 教師データ（one-hot vector)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size # delta3に相当\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### アフィン変換層 (xW + b)\n",
    "\n",
    "更新手法を\n",
    "- sgd\n",
    "- adagrad\n",
    "- adam\n",
    "\n",
    "と切り替えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine(Layer):\n",
    "    def __init__(self, W, b, params= {}):\n",
    "        super(Affine, self).__init__(params)\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        # パラメータの微分値\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        # 学習率のセット\n",
    "        if 'lr' in params:\n",
    "            self.lr = params['lr']\n",
    "        else:\n",
    "            self.lr = 0.01\n",
    "        \n",
    "        # 更新式のスイッチング\n",
    "        # optimizeメソッドをoptimizerによって切り替える。\n",
    "        if params['optimizer']=='sgd':\n",
    "            self.optimize = self.update_sgd\n",
    "        elif params['optimizer'] == 'adagrad':\n",
    "            self.h = np.zeros_like(W)\n",
    "            self.optimize = self.update_adagrad\n",
    "        else: # params['optimizer'] == 'adam':\n",
    "            self.m = np.zeros_like(W)\n",
    "            self.v = np.zeros_like(W)\n",
    "            self.beta1 = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.optimize = self.update_adam\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def update_sgd(self):\n",
    "        self.W -= self.lr * self.dW\n",
    "        self.b -= self.lr * self.db\n",
    "    \n",
    "    # adagrad 少しずつ更新量が減っていく\n",
    "    def update_adagrad(self, lr = 0.01):\n",
    "        self.h += self.dW ** 2\n",
    "        self.W -= self.lr * self.dW / (np.sqrt(self.h) + 1e-7)\n",
    "        self.b -= self.lr * self.db\n",
    "        \n",
    "    \n",
    "    def update_adam(self, lr = 0.01):\n",
    "        self.m = self.beta1 * self.m + (1- self.beta1) * self.dW\n",
    "        self.v = self.beta2 * self.v + (1- self.beta2) * (self.dW * self.dW)\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1)\n",
    "        v_hat = self.v / (1 - self.beta2)\n",
    "        \n",
    "        self.W -= self.lr * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "        self.b -= self.lr * self.db\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数層\n",
    "\n",
    "活性化関数層は\n",
    "\n",
    "- ReLU\n",
    "- tanh\n",
    "- シグモイド関数\n",
    "\n",
    "を切り替えて使用できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    '''\n",
    "    活性化関数を設定できる \n",
    "    'tanh'\n",
    "    'sigmoid'\n",
    "    'relu'\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super(Activation, self).__init__(params)\n",
    "        self.out = None\n",
    "        self.mask = None\n",
    "        # optimizeメソッドを\n",
    "        if params['activation']=='tanh':\n",
    "            self.forward = self.forward_tanh\n",
    "            self.backward = self.backward_tanh\n",
    "        elif params['activation'] == 'sigmoid':\n",
    "            self.forward = self.forward_sigmoid\n",
    "            self.backward = self.backward_sigmoid\n",
    "        else: # params['activation'] == 'relu':\n",
    "            self.forward = self.forward_relu\n",
    "            self.backward = self.backward_relu\n",
    "     \n",
    "    def forward_relu(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward_relu(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "\n",
    "    # tanh \n",
    "    def forward_tanh(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward_tanh(self, dout):\n",
    "        dx = dout * (1 - np.tanh(dout)**2)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    # sigmoid関数\n",
    "    def forward_sigmoid(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward_sigmoid(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチノーマリゼーション層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    def __init__(self, params):\n",
    "        super(BatchNorm, self).__init__(params)\n",
    "        self.out = None\n",
    "        self.beta = 0.0\n",
    "        self.gamma = 1.0\n",
    "        self.lr = params['lr']\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "        '''\n",
    "        計算式は下記を参照\n",
    "        https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        #data_size, input_size = x.shape\n",
    "        \n",
    "        # 単に標準化する\n",
    "        # out = (x - np.mean(x, axis=0)) / np.var()\n",
    "        \n",
    "        # step1: 平均を求める\n",
    "        mu = np.mean(x, axis=0)\n",
    "        \n",
    "        # step2: 偏差\n",
    "        self.xmu = x - mu\n",
    "        \n",
    "        # step3 : 偏差の２乗\n",
    "        sq = self.xmu ** 2\n",
    "        \n",
    "        # step4 : 分散を求める\n",
    "        self.var = np.var(x, axis=0)\n",
    "        \n",
    "        # step5 : 分散のルートを取った値を求める\n",
    "        self.sqrtvar = np.sqrt(self.var + self.eps)\n",
    "        \n",
    "        # step6 : sqrtvarの逆数（invert）\n",
    "        self.ivar = 1.0/ self.sqrtvar\n",
    "        \n",
    "        # step7 : 標準化した値\n",
    "        self.xhat = self.xmu * self.ivar\n",
    "        \n",
    "        # step8\n",
    "        gammax = self.gamma * self.xhat\n",
    "        \n",
    "        # step9\n",
    "        out = gammax + self.beta\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        #get the dimensions of the input/output\n",
    "        N, D = dout.shape\n",
    "        \n",
    "        # step9\n",
    "        self.d_beta = np.sum(dout, axis=0)\n",
    "        dgammax = dout #not necessary, but more understandable\n",
    "        \n",
    "        # step8\n",
    "        self.d_gamma = np.sum(dgammax*self.xhat, axis=0)\n",
    "        dxhat = dgammax * self.gamma\n",
    "        \n",
    "        # step7\n",
    "        divar = np.sum(dxhat*self.xmu, axis=0)\n",
    "        dxmu1 = dxhat * self.ivar\n",
    "        \n",
    "        # step6\n",
    "        dsqrtvar = -1. /(self.sqrtvar**2) * divar\n",
    "        \n",
    "        # step5\n",
    "        dvar = 0.5 * 1. / np.sqrt(self.var+self.eps) * dsqrtvar\n",
    "        \n",
    "        # step4\n",
    "        dsq = 1. / N * np.ones((N, D)) * dvar\n",
    "        \n",
    "        # step3\n",
    "        dxmu2 = 2 * self.xmu * dsq\n",
    "        \n",
    "        # step2\n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "        dmu = -1 * np.sum(dxmu1 + dxmu2, axis=0)\n",
    "        \n",
    "        # step1\n",
    "        dx2 = 1. / N * np.ones((N, D)) * dmu\n",
    "        \n",
    "        # step0\n",
    "        dx = dx1 + dx2\n",
    "        \n",
    "        return dx\n",
    "\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.gamma -= self.lr * self.d_gamma\n",
    "        self.beta -= self.lr * self.d_beta\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ドロップアウト層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, params):\n",
    "        super(Dropout, self).__init__(params)\n",
    "        if 'dropout_ratio' in params:\n",
    "            self.dropout_ratio = params['dropout_ratio']\n",
    "        else:\n",
    "            self.dropout_ratio = 0.5\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg :\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1 - self.dropout_ratio)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 複数層を束ねるクラス(ネットワーククラス)\n",
    "\n",
    "改良したいところ\n",
    "\n",
    "addで層を追加（好きに追加できる。）\n",
    "\n",
    "各層の親クラスを作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Layers:\n",
    "    def __init__(self, params):\n",
    "        unit_size_list = [params['input_size']]\n",
    "        unit_size_list.extend(params['hidden_layer_list'])\n",
    "        unit_size_list.append(params['output_size'])\n",
    "        \n",
    "        self.params = {}\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        # とりあえずここにべた書きで書けるようにする\n",
    "        \n",
    "#         for i in range(1, len(unit_size_list)):\n",
    "#             # 重みの初期化\n",
    "#             init_W = np.random.randn(unit_size_list[i-1], unit_size_list[i])\n",
    "#             init_b = np.zeros([1, unit_size_list[i]])\n",
    "#             if params['init'] == 'gauss':\n",
    "#                 init_W *= 0.01\n",
    "#             elif params['init'] == 'xavier':\n",
    "#                 init_W /= np.sqrt(unit_size_list[i-1])\n",
    "#             else: # He\n",
    "#                 init_W = init_W / np.sqrt(unit_size_list[i-1]) * np.sqrt(2) \n",
    "                \n",
    "#             # アフィン変換層（Wx + b）を追加する\n",
    "#             self.layers['Affine' + str(i)] = Affine(init_W, init_b, params)\n",
    "            \n",
    "#             # 最終層以外はバッチノーマリゼーション層と活性化関数層を追加する\n",
    "#             if i < (len(unit_size_list)-1):\n",
    "#                 if params['batch_norm'] == True:\n",
    "#                     self.layers['BatchNorm' + str(i)] = BatchNorm(params)\n",
    "#                 self.layers['Active' + str(i)] = Activation(params)\n",
    "# #                 if params['dropout_ratio'] > 0:\n",
    "# #                     self.layers['Dropout' + str(i)] = Dropout(params)\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "        # self.params['hidden_layer_num'] = len(unit_size_list)-1\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # forwardを繰り返す\n",
    "        # ソフトマックスを通さなくても答えは出るのでこれで予測とする \n",
    "        # argmaxでラベルを取れる\n",
    "        for layer in self.layers.values():\n",
    "            x =layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        # 正答率を小数点第二桁で出力する\n",
    "        y_pred = self.predict(x)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(t, axis=1)\n",
    "        data_size = x.shape[0]\n",
    "\n",
    "        correct_count = np.sum([y_true == y_pred]) \n",
    "        score = correct_count / data_size * 100\n",
    "\n",
    "        return round(score, 2)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    \n",
    "    def optimize(self, x, t):\n",
    "        \n",
    "        # forward \n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.lastLayer.backward(1)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # optimizeメソッドがある層は更新を行う\n",
    "        # AffineとBatchNorm層のみ行うはず\n",
    "        for layer in self.layers.values():\n",
    "            if hasattr(layer, \"optimize\"):\n",
    "                layer.optimize()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO : Gradient Checkを導入する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, init='gauss', iteration = 500, lr = 0.05,  lam = 0.01, \n",
    "                 batch_mode = 'mini', activation='relu',\n",
    "                 batch_size_rate = 0.1, hidden_layer_list = [5], optimizer='sgd',\n",
    "                 batch_norm=False, dropout_ratio=0.0):\n",
    "        \"\"\" ハイパーパラメータ解説\n",
    "        init: 初期化方法\n",
    "            'he' : \n",
    "            'gauss' \n",
    "            'xavier'\n",
    "        lr : 学習率\n",
    "        lam : 正則化項の率\n",
    "        batch_size: バッチサイズ\n",
    "            'batch' : フルサイズ\n",
    "            'mini' 0< x< 1: フルサイズ割合 0.1なら全体の0.1サイズ使用する\n",
    "            'online' : オンライン学習　１データのみ\n",
    "        hidden_layer_list : 隠れ層のリスト、層のユニットをリストで入力　例[2, 3]　ユニット数２、ユニット数３の隠れ層\n",
    "        optimizer : 勾配の更新手法\n",
    "            'sgd' : 確率的勾配降下法\n",
    "            'adam': \n",
    "            'adagrad':\n",
    "        activation: 活性化関数の名前\n",
    "            'relu' : ReLU関数\n",
    "            'tanh' : tanh\n",
    "            'sigmoid' : シグモイド関数\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['iteration'] = iteration\n",
    "        self.params['init'] = init\n",
    "        self.params['lr'] = lr\n",
    "        self.params['lam'] = lam # 正則化項用の係数　今は使っていない\n",
    "        self.params['batch_mode'] = batch_mode # データ数が決まったらそれに基づいて変更する\n",
    "        self.params['batch_size_rate'] = batch_size_rate # ミニバッチ法のときのみ使用する\n",
    "        self.params['hidden_layer_list'] = hidden_layer_list\n",
    "        self.params['optimizer'] = optimizer\n",
    "        self.params['batch_norm'] = batch_norm\n",
    "        self.params['dropout_ratio'] = dropout_ratio\n",
    "        self.params['activation'] = activation # 活性化関数\n",
    "        \n",
    "    def train(self, X, y, params={}):\n",
    "        # 入力パラメータがあれば更新する\n",
    "        for key in params:\n",
    "                self.params[key] = params[key]\n",
    "        \n",
    "        # 正規化　必要？\n",
    "        X = X / 255.0\n",
    "        \n",
    "        # 訓練とテストデータに分割\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        self.params['data_size'] = X_train.shape[0]\n",
    "        self.params['input_size'] = X_train.shape[1]\n",
    "        self.params['output_size'] = y_train.shape[1]\n",
    "\n",
    "        \n",
    "        # コストや正答率の学習曲線を引くためのリストを用意\n",
    "        past_train_costs = []\n",
    "        past_test_costs = []\n",
    "        past_train_accuracy = []\n",
    "        past_test_accuracy = []\n",
    "        \n",
    "        # 初期化\n",
    "        # 重み初期化\n",
    "        # バッチサイズの設定\n",
    "        if self.params['batch_mode'] == 'batch':\n",
    "            self.params['batch_size'] = self.params['data_size']\n",
    "        elif self.params['batch_mode'] == 'mini':\n",
    "            self.params['batch_size'] = int(self.params['data_size']  * self.params['batch_size_rate'] ) \n",
    "        else:\n",
    "            self.params['batch_size'] = 1\n",
    "        # 隠れ層やレイヤーインスタンス生成\n",
    "        self.params['layer'] = Layers(self.params)\n",
    "        \n",
    "        \n",
    "        # 何イテレーションで1エポックか\n",
    "        epoch_per_i = int(self.params['data_size'] / self.params['batch_size'])\n",
    "        \n",
    "        ##################\n",
    "        # 最急降下法での学習\n",
    "        ##################\n",
    "        for i in range(self.params['iteration']):\n",
    "            \n",
    "            # 学習に使用するデータをサンプリング\n",
    "            choice_index = np.random.choice(self.params['data_size'], self.params['batch_size'])\n",
    "            X_batch, y_batch = X_train[choice_index], y_train[choice_index]\n",
    "            \n",
    "            # 誤差逆伝播法によって勾配を求め、値を更新\n",
    "            self.params['layer'].optimize(X_batch, y_batch)\n",
    "            \n",
    "            # 1エポックごとに正答率とコストを算出して保存する\n",
    "            if i % epoch_per_i == 0:              \n",
    "                past_train_accuracy.append(self.params['layer'].accuracy(X_train, y_train))\n",
    "                past_test_accuracy.append(self.params['layer'].accuracy(X_test, y_test))\n",
    "                \n",
    "                past_train_costs.append(self.params['layer'].loss(X_train, y_train))\n",
    "                past_test_costs.append(self.params['layer'].loss(X_test, y_test))\n",
    "            \n",
    "        return past_train_accuracy, past_test_accuracy, past_train_costs, past_test_costs \n",
    "    \n",
    "\n",
    "        \n",
    "    # 現在のパラメータで予測値を確率かラベルで出力する。\n",
    "    def predict(self, X, probability=False):\n",
    "        predict = self.params['layer'].predict(X, train_flg=False)\n",
    "        predict_proba = softmax(predict)\n",
    "        if probability== True:\n",
    "            return predict_proba\n",
    "        else:\n",
    "            return np.argmax(predict_proba, axis=1)\n",
    "        \n",
    "    def plot_learning_curve(self, X, y, metrics='acc', params={}): \n",
    "        past_train_accuracy, past_test_accuracy, past_train_costs, past_test_costs = self.train(X, y, params)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        # count_epoch = self.params['iteration'] // self.params['data_size'] + 1\n",
    "        if metrics == 'cost':\n",
    "            plt.plot(past_train_costs, color='orange', label='train')\n",
    "            plt.plot(past_test_costs, color='lime', label='test')\n",
    "            plt.ylabel(\"cost\", fontsize=15)\n",
    "            print(\"last train cost is {}\".format(past_train_costs[-1]))\n",
    "            print(\"last test cost is {}\".format(past_test_costs[-1]))\n",
    "        else:\n",
    "            #plt.plot(np.array(past_train_accuracy), color='r')\n",
    "            plt.plot(past_train_accuracy, color='orange', label='train')\n",
    "            plt.plot(past_test_accuracy, color='lime', label='test')\n",
    "            plt.ylabel(\"accuracy\", fontsize=15)\n",
    "            print(\"last train accuracy is {}\".format(past_train_accuracy[-1]))\n",
    "            print(\"last test accuracy is {}\".format(past_test_accuracy[-1]))\n",
    "            plt.ylim(-0.5, 100.5)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title('Learning Curve', fontsize=20)\n",
    "        plt.xlabel(\"iteration[epoch]\", fontsize=15)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プロットはクラスに入れず、trainの返り値を使ってplotすればよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(iteration=10, optimizer='adam', \n",
    "            hidden_layer_list = [100], batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'optimizer': 'adam',\n",
    "#          'batch_mode' : 'mini',\n",
    "#          'init': 'he',\n",
    "#          'lr': 0.007}\n",
    "\n",
    "# #past_train_accuracy, past_test_accuracy = model.train(X,y, params)\n",
    "# # model.plot_learning_curve(X,y, 'cost', params)\n",
    "\n",
    "# model.plot_learning_curve(x_train,y_train, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提出用ファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コメントアウトを外せば提出ファイルができる。\n",
    "# test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# X_sub = test_df\n",
    "# X_sub = np.array(X_sub)\n",
    "\n",
    "# # train_flgでエラー中　すべてparamsに押し込めれば良い\n",
    "# Y_pred = model.predict(X_sub)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#        \"ImageId\": np.array(test_df.index) + 1,\n",
    "#        \"Label\": Y_pred\n",
    "#    })\n",
    "\n",
    "# submission.to_csv(\"./submission_001.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入出力サイズ対応が必要か\n",
    "\n",
    "### DNNクラス\n",
    "\n",
    "### Layersクラス\n",
    "\n",
    "### 単層クラス\n",
    "\n",
    "- Affine層\n",
    "    - SGD\n",
    "    - Adam\n",
    "    - adgrad\n",
    "- 活性化関数\n",
    "    - tanh\n",
    "    - ReLU\n",
    "    - sigmoid\n",
    "- バッチノーマリゼーション層\n",
    "- ドロップアウト層(要修正)\n",
    "\n",
    "\n",
    "- 畳み込み層　（forward確認中）\n",
    "- プーリング層 （forward、　）\n",
    "- Flatten層 (済)\n",
    "\n",
    "im2col col2im\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['activation'] = 'relu'\n",
    "act1 = Activation(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 28, 28)\n",
      "(80, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(act1.forward(x_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 32, 32, 3)\n",
      "(100, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_cifar10.shape)\n",
    "print(act1.forward(x_train_cifar10).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数はサイズを気にしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cifar10[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattenクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "    # (N, H, W, C)のデータを\n",
    "    # (N, H * W * C)に変更する\n",
    "    def __init__(self, params={}):\n",
    "        super(Flatten, self).__init__(params)\n",
    "        # 入出力サイズ以外のパラメータはない\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input_shape = x.shape\n",
    "        out = np.array([elem.flatten() for elem in x_train])\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.reshape(self.input_shape)\n",
    "        \n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 28, 28, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:, :, :, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 784)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat = Flatten()\n",
    "forward_val = flat.forward(x_train[:, :, :, np.newaxis])\n",
    "forward_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 28, 28, 1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat.backward(forward_val).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力が変わらないパディング\n",
    "\n",
    "フィルターサイズが奇数であることを条件に、\n",
    "フィルターサイズを２で割った商をパディング数とすればよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "(100, 34, 34, 3)\n"
     ]
    }
   ],
   "source": [
    "filter_size = 3\n",
    "\n",
    "pad = filter_size // 2\n",
    "\n",
    "print(x_train_cifar10[0].shape)\n",
    "      \n",
    "print(np.pad(x_train_cifar10,[(0,0),(pad, pad),(pad, pad),(0,0)],'constant').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 同じように'edge'にすればパディングは隣接値をそのまま使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 34, 34, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.pad(x_train_cifar10,[(0,0),(pad, pad),(pad, pad),(0,0)],'edge').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### im2colの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### これをim2colの反対のcol2imで元のデータ形状に戻してやれば良い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(x, filter_size=4, stride=4, pad=0, padding='constant'):\n",
    "    # 4次元データを2次元データに変形する\n",
    "    # XXX: もしかしたらできているかも？？　今の所パディングして数が変わるとエラーになる\n",
    "    # TODO: 上記を修正して、畳み込み層にも使えるように修正する\n",
    "    \n",
    "    # パディングして外堀を埋める\n",
    "    if pad > 0: \n",
    "        x = np.pad(x,[(0,0),(pad, pad),(pad, pad),(0,0)], padding)\n",
    "\n",
    "    # データ数　高さ　幅　チャネルを取得\n",
    "    N, H, W, C = x.shape\n",
    "    \n",
    "    # それぞれのブロックごとにflatten\n",
    "    return np.array([x[n, h:h+filter_size, w:w+filter_size, c].flatten() for c in range(C) for n in range(N) \\\n",
    "            for h in range(0, H-filter_size+1, stride) for w in range(0, W-filter_size+1, stride)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im2colの逆をやる関数\n",
    "def col2im(x, input_shape, filter_size=4, stride=4, pad=0, padding='constant'):\n",
    "    \n",
    "    # 戻すデータの形状\n",
    "    N, H, W, C = input_shape\n",
    "    \n",
    "    # リターン箱を作る\n",
    "    img = np.zeros(input_shape)\n",
    "    # ブロック数\n",
    "    block_num = (pad * 2 + W - filter_size)/ stride + 1 # 本当は高さ　幅別々に計算する\n",
    "\n",
    "    # １チャネルに何行あるか\n",
    "    c_vol = int(ret.shape[0] / C)\n",
    "    n_vol = int(c_vol / N)\n",
    "\n",
    "    for i, line in enumerate(ret):\n",
    "        # 帯からブロックにする\n",
    "        block = line.reshape(4, 4)\n",
    "\n",
    "        channel_i = i//c_vol\n",
    "\n",
    "        data_i = i % c_vol // n_vol\n",
    "\n",
    "        start_h, start_w = divmod(i % n_vol, block_num) # 本当はblock_h\n",
    "        start_h, start_w = int(start_h), int(start_w)\n",
    "\n",
    "        end_h = start_h + 4 # filter_size or filter_height\n",
    "        end_w = start_w + 4 # filter_size or filter_weight\n",
    "\n",
    "        img[data_i, start_h:end_h, start_w:end_w, channel_i] = block\n",
    "        \n",
    "    return img[:, pad:pad+H, pad:pad+W, :]                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32, 3)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = im2col(x_train_cifar10[:10], pad=1)\n",
    "x_train_cifar10[:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 16)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32, 3)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col2im(ret, x_train_cifar10[:10].shape).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### col2im im2colが完成！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPoolingクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(Layer):\n",
    "    \n",
    "    def __init__(self, pool_size=4, stride=-1, pad=0, params={}):\n",
    "        super(MaxPooling, self).__init__(params)\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.stride = self.pool_size if self.stride == -1 else stride\n",
    "        # パディング方法をしてできたほうがよい？　今はゼロ埋め固定\n",
    "        # 最大値の場所を保存して、backwardでその場所以外は伝播しない\n",
    "        self.max_index = None\n",
    "        self.input_shape = None # 親クラスで定義するのでここには後々必要なくなる\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.input_shape = x.shape\n",
    "        N, H, W, C = x.shape\n",
    "        \n",
    "        col = im2col(x, self.pool_size, self.pool_size, self.pad)\n",
    "        col_max = np.max(col, axis=1)\n",
    "        self.max_index = np.argmax(col, axis=1)\n",
    "#       print(col_max.shape)\n",
    "#       col_max = np.max(im2col(x, self.pool_size, self.stride, self.pad, 'constant'), axis=1)\n",
    "        \n",
    "        # 出力サイズを確認\n",
    "        out_h = (H + 2*self.pad - self.pool_size)// self.stride + 1 \n",
    "        out_w = (W + 2*self.pad - self.pool_size)// self.stride + 1 \n",
    "        \n",
    "        # 整形\n",
    "        return col_max.reshape(C, N, out_h, out_w).transpose(1, 2, 3, 0)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout_line = dout.transpose(3, 0, 1, 2).reshape(-1)\n",
    "        filter_size = 4\n",
    "        \n",
    "        # 返り値の箱を作る\n",
    "        ret = np.zeros([dout_line.shape[0],self.pool_size* self.pool_size])\n",
    "        \n",
    "        # 最大値の場所にdoutを流し込む\n",
    "        for i, max_i in enumerate(poollayer.max_index):\n",
    "            ret[i, max_i] = dout_line[i]\n",
    "            \n",
    "        # 元の形状に戻してリターン\n",
    "        return col2im(ret, self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(Layer):\n",
    "    \n",
    "    def __init__(self, out_channel = 1, filter_size=3, stride=1, pad=0, bias=True, params={}):\n",
    "        # biasなしに対応していない。 dbを更新しなければよい？\n",
    "        self.out_channel =out_channel\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.bias = bias\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        ##### Affineからパクリ ########\n",
    "        self.x = None\n",
    "        # パラメータの微分値\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.optimize = None\n",
    "        # 学習率のセット\n",
    "        if 'lr' in params:\n",
    "            self.lr = params['lr']\n",
    "        else:\n",
    "            self.lr = 0.01\n",
    "        self.x_2dim = None # im2col後のxの値を保持\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape # N H W Cで来ると想定\n",
    "        N, H, W, C = in_shape\n",
    "        # 出力の高さと幅を計算する\n",
    "        out_h = 1 + int((H + 2*self.pad - self.filter_size) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - self.filter_size) / self.stride)\n",
    "        \n",
    "        self.out_shape = (N, out_h, out_w, self.out_channel)\n",
    "        \n",
    "        # 重みの初期化\n",
    "        # im2colを見越して２次元の重みとして実装\n",
    "        self.W = np.random.randn(self.filter_size * self.filter_size * self.in_shape[3], self.out_channel)\n",
    "        self.b = np.zeros([self.out_channel, 1])\n",
    "        # TODO 初期化を選択できるように設定する　ひとまずはガウス初期化\n",
    "#         if params['init'] == 'gauss':\n",
    "        self.W *= 0.01\n",
    "#         elif params['init'] == 'xavier':\n",
    "#             # 入力層のユニット数 N * out_h * out_w ? \n",
    "#             self.W /= np.sqrt(self.out_shape[0] * self.out_shape[1] * self.out_shape[2])\n",
    "#         else: # He\n",
    "#             self.W = self.W / np.sqrt(self.out_shape[0] * self.out_shape[1] * self.out_shape[2]) * np.sqrt(2) \n",
    "        \n",
    "        # 更新式のスイッチング\n",
    "        # optimizeメソッドをoptimizerによって切り替える。\n",
    "#         if params['optimizer']=='sgd':\n",
    "#             self.optimize = self.update_sgd\n",
    "#         elif params['optimizer'] == 'adagrad':\n",
    "#             self.h = np.zeros_like(W)\n",
    "#             self.optimize = self.update_adagrad\n",
    "#         else: # params['optimizer'] == 'adam':\n",
    "#             self.m = np.zeros_like(W)\n",
    "#             self.v = np.zeros_like(W)\n",
    "#             self.beta1 = 0.9\n",
    "#             self.beta2 = 0.999\n",
    "#             self.optimize = self.update_adam\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_C = self.in_shape[3]\n",
    "        out_C = self.out_shape[3]\n",
    "        N = self.in_shape[0]\n",
    "        \n",
    "        # 出力の高さと幅を計算する\n",
    "        out_h = 1 + int((H + 2*self.pad - self.filter_size) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - self.filter_size) / self.stride)\n",
    "        \n",
    "        print(\"IN {} \".format(x.shape))\n",
    "        # ２次元配列に変換する\n",
    "        # (out_h * out_w * N * C, filter_size*filter_size)\n",
    "        x_2dim = im2col(x, self.filter_size, self.stride, self.pad)\n",
    "        self.x_2dim = x_2dim\n",
    "        print(\"after im2col {} \".format(x_2dim.shape))\n",
    "        \n",
    "        # 各色（チャネル）ごとに\n",
    "        W_color = self.W.reshape(in_C, -1, out_C)\n",
    "        print(\"W {} \".format(self.W.shape))\n",
    "        print(\"W_color {} \".format(W_color.shape))\n",
    "        x_2dim_color = x_2dim.reshape(in_C, -1, out_h * out_w * N)\n",
    "        print(\"x_2dim_color {} \".format(x_2dim_color.shape))\n",
    "        \n",
    "        out = np.zeros((in_C, out_C, out_h * out_w * N))\n",
    "        for c in range(in_C):\n",
    "            out[c] = np.dot(W_color[c].T, x_2dim_color[c])\n",
    "        \n",
    "        print(\"out before sum {} \".format(out.shape))\n",
    "        # RGBごとの結果を合計する\n",
    "        out = np.sum(out, axis=0)\n",
    "        print(\"out after sum {} \".format(out.shape))\n",
    "        \n",
    "        out = out + self.b\n",
    "        \n",
    "        # 整形する　out_c N H W\n",
    "        return out.reshape(out_C, N, out_h, out_w).transpose(1, 2 ,3, 0)\n",
    "        \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        out_c = self.out_shape[3]\n",
    "        print(\"dout {} \".format(self.out_shape))\n",
    "        dout = dout.reshape(-1, out_c)\n",
    "        print(\"dout after reshape{} \".format(dout.shape))\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)[:, np.newaxis]\n",
    "        print(\"db after sum{} \".format(self.db.shape))\n",
    "        print(\"d {}\".format(self.b.shape))\n",
    "        print(\"W {} \".format(self.W.shape))\n",
    "        print(\"X shape {} \".format(self.x.shape))\n",
    "        #self.dW = \n",
    "    \n",
    "    \n",
    "    #####affineからパクリ\n",
    "    def update_sgd(self):\n",
    "        self.W -= self.lr * self.dW\n",
    "        self.b -= self.lr * self.db\n",
    "    \n",
    "    \n",
    "    # adagrad 少しずつ更新量が減っていく\n",
    "    def update_adagrad(self, lr = 0.01):\n",
    "        self.h += self.dW ** 2\n",
    "        self.W -= self.lr * self.dW / (np.sqrt(self.h) + 1e-7)\n",
    "        self.b -= self.lr * self.db\n",
    "        \n",
    "    \n",
    "    def update_adam(self, lr = 0.01):\n",
    "        self.m = self.beta1 * self.m + (1- self.beta1) * self.dW\n",
    "        self.v = self.beta2 * self.v + (1- self.beta2) * (self.dW * self.dW)\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1)\n",
    "        v_hat = self.v / (1 - self.beta2)\n",
    "        \n",
    "        self.W -= self.lr * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "        self.b -= self.lr * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forwardメソッドの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(6).reshape(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(a, (3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN (10, 32, 32, 3) \n",
      "after im2col (27000, 9) \n",
      "W (27, 2) \n",
      "W_color (3, 9, 2) \n",
      "x_2dim_color (3, 9, 9000) \n",
      "out before sum (3, 2, 9000) \n",
      "out after sum (2, 9000) \n"
     ]
    }
   ],
   "source": [
    "conv = Convolution(2, 3, 1, 0)\n",
    "data = x_train_cifar10[:10]\n",
    "conv.initialize(data.shape)\n",
    "out = conv.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dout (10, 30, 30, 2) \n",
      "dout after reshape(9000, 2) \n",
      "db after sum(2, 1) \n",
      "d (2, 1)\n",
      "W (27, 2) \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-416-6c3d86cfaec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-413-72f5343e1879>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"d {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X shape {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;31m#self.dW =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "conv.backward(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 2)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32, 3)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.in_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32, 3)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30, 30, 2)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 2)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.reshape(-1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
       "        46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = np.arange(60)\n",
    "data = col.reshape(2, -1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0, 30],\n",
       "        [ 1, 31],\n",
       "        [ 2, 32]],\n",
       "\n",
       "       [[ 3, 33],\n",
       "        [ 4, 34],\n",
       "        [ 5, 35]]])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out_c N H W\n",
    "data.reshape(2, 5, 2, 3).transpose(1, 2 ,3, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_v2(x, filter_size=4, stride=4, pad=0, padding='constant'):\n",
    "    # 4次元データを2次元データに変形する\n",
    "    # XXX: もしかしたらできているかも？？　今の所パディングして数が変わるとエラーになる\n",
    "    # TODO: 上記を修正して、畳み込み層にも使えるように修正する\n",
    "    \n",
    "    # パディングして外堀を埋める\n",
    "    if pad > 0: \n",
    "        x = np.pad(x,[(0,0),(pad, pad),(pad, pad),(0,0)], padding)\n",
    "\n",
    "    # データ数　高さ　幅　チャネルを取得\n",
    "    N, H, W, C = x.shape\n",
    "    \n",
    "    # それぞれのブロックごとにflatten\n",
    "    return np.array([x[n, h:h+filter_size, w:w+filter_size, c].flatten() for n in range(N) for c in range(C)  \\\n",
    "            for h in range(0, H-filter_size+1, stride) for w in range(0, W-filter_size+1, stride)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 16)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = x_train_cifar10[:1]\n",
    "im2col_v2(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8, 8, 3)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_max = np.max(im2col(x_train_cifar10[:10], 4, 4, 0, 'constant'), axis=1)\n",
    "col_max.reshape(3, 10, 8, 8).transpose(1, 2, 3, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8, 8, 3)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "poollayer = MaxPooling()\n",
    "poollayer.forward(x_train_cifar10[:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 8, 8, 3)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poollayer = MaxPooling(pad=0)\n",
    "print(x_train_cifar10[:10].shape)\n",
    "dout = poollayer.forward(x_train_cifar10[:10])\n",
    "dout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32, 3)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = poollayer.backward(dout)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPoolingの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(16).reshape(1,4,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 4, 1)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[15]]]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poollayer.forward(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxPoolingのbackward関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = poollayer.forward(x_train_cifar10[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8, 8, 3)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_max.reshape(C, N, out_h, out_w).transpose(1, 2, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 64)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dout.transpose(3, 0, 1, 2).reshape(-1, 64).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 8, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "poollayer = MaxPooling()\n",
    "dout = poollayer.forward(x_train_cifar10[:10])\n",
    "print(dout.shape)\n",
    "\n",
    "dout_line = dout.transpose(3, 0, 1, 2).reshape(-1)\n",
    "\n",
    "\n",
    "filter_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106., 106., 106., 106.],\n",
       "       [145., 145., 145., 145.],\n",
       "       [149., 149., 149., 149.],\n",
       "       ...,\n",
       "       [ 62.,  62.,  62.,  62.],\n",
       "       [ 79.,  79.,  79.,  79.],\n",
       "       [ 86.,  86.,  86.,  86.]])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones([dout.transpose(3, 0, 1, 2).reshape(-1).shape[0],filter_size]) * dout.transpose(3, 0, 1, 2).reshape(-1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 16)\n"
     ]
    }
   ],
   "source": [
    "modosu = np.ones([dout.transpose(3, 0, 1, 2).reshape(-1).shape[0],filter_size*filter_size]) * dout.transpose(3, 0, 1, 2).reshape(-1)[:, np.newaxis]\n",
    "print(modosu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30720"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modosu.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)            (None, 28, 28, 2)         20        \n",
    "_________________________________________________________________\n",
    "batch_normalization_1 (Batch (None, 28, 28, 2)         8         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 2)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 14, 14, 2)         38        \n",
    "_________________________________________________________________\n",
    "batch_normalization_2 (Batch (None, 14, 14, 2)         8         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 2)           0         \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 7, 7, 2)           38        \n",
    "_________________________________________________________________\n",
    "batch_normalization_3 (Batch (None, 7, 7, 2)           8         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 2)           0         \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 18)                0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 128)               2432      \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 128)               16512     \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 10)                1290  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class LeNetLayers:\n",
    "    def __init__(self, params):\n",
    "        unit_size_list = [params['input_size']]\n",
    "        unit_size_list.extend(params['hidden_layer_list'])\n",
    "        unit_size_list.append(params['output_size'])\n",
    "        \n",
    "        self.params = {}\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        # とりあえずここにべた書きで書けるようにする\n",
    "        # self.layer['Conv1'] = Convolution() # 2 (3,3) (1,1) same\n",
    "        # self.layer['BatchNorm1'] = BatchNorm(params)\n",
    "        # self.layer['Active1'] = Activation(params) # relu\n",
    "        # self.layer['Pool1'] = MaxPooling(params, 2, 2, 0) \n",
    "        \n",
    "        # self.layer['Conv2'] = Convolution() # 2 (3,3) (1,1) same\n",
    "        # self.layer['BatchNorm2'] = BatchNorm(params)\n",
    "        # self.layer['Active2'] = Activation(params) # relu\n",
    "        # self.layer['Pool2'] = MaxPooling(params, 2, 2, 0) \n",
    "        \n",
    "        # self.layer['Conv3'] = Convolution() # 2 (3,3) (1,1) same\n",
    "        # self.layer['BatchNorm3'] = BatchNorm(params)\n",
    "        # self.layer['Active3'] = Activation(params) # relu\n",
    "        # self.layer['Pool3'] = MaxPooling(params, 2, 2, 0) \n",
    "        \n",
    "        \n",
    "#         for i in range(1, len(unit_size_list)):\n",
    "#             # 重みの初期化\n",
    "#             init_W = np.random.randn(unit_size_list[i-1], unit_size_list[i])\n",
    "#             init_b = np.zeros([1, unit_size_list[i]])\n",
    "#             if params['init'] == 'gauss':\n",
    "#                 init_W *= 0.01\n",
    "#             elif params['init'] == 'xavier':\n",
    "#                 init_W /= np.sqrt(unit_size_list[i-1])\n",
    "#             else: # He\n",
    "#                 init_W = init_W / np.sqrt(unit_size_list[i-1]) * np.sqrt(2) \n",
    "                \n",
    "#             # アフィン変換層（Wx + b）を追加する\n",
    "#             self.layers['Affine' + str(i)] = Affine(init_W, init_b, params)\n",
    "            \n",
    "#             # 最終層以外はバッチノーマリゼーション層と活性化関数層を追加する\n",
    "#             if i < (len(unit_size_list)-1):\n",
    "#                 if params['batch_norm'] == True:\n",
    "#                     self.layers['BatchNorm' + str(i)] = BatchNorm(params)\n",
    "#                 self.layers['Active' + str(i)] = Activation(params)\n",
    "# #                 if params['dropout_ratio'] > 0:\n",
    "# #                     self.layers['Dropout' + str(i)] = Dropout(params)\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "        # self.params['hidden_layer_num'] = len(unit_size_list)-1\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # forwardを繰り返す\n",
    "        # ソフトマックスを通さなくても答えは出るのでこれで予測とする \n",
    "        # argmaxでラベルを取れる\n",
    "        for layer in self.layers.values():\n",
    "            x =layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        # 正答率を小数点第二桁で出力する\n",
    "        y_pred = self.predict(x)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(t, axis=1)\n",
    "        data_size = x.shape[0]\n",
    "\n",
    "        correct_count = np.sum([y_true == y_pred]) \n",
    "        score = correct_count / data_size * 100\n",
    "\n",
    "        return round(score, 2)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    \n",
    "    def optimize(self, x, t):\n",
    "        \n",
    "        # forward \n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.lastLayer.backward(1)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # optimizeメソッドがある層は更新を行う\n",
    "        # AffineとBatchNorm層のみ行うはず\n",
    "        for layer in self.layers.values():\n",
    "            if hasattr(layer, \"optimize\"):\n",
    "                layer.optimize()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
