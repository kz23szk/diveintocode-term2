{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNNスクラッチコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのロード\n",
    "\n",
    "mnistの手書き文字認識問題\n",
    "\n",
    "784ピクセルの値（0-255）から数字（0-9）を分類する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kzfm/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# # プロトタイプなので100データだけ使用する\n",
    "# x_train = x_train[:100]\n",
    "# y_train = y_train[:100]\n",
    "# y_label = y_train\n",
    "# y_train = np.identity(10)[y_train]\n",
    "# del x_test, y_test\n",
    "\n",
    "\n",
    "# # 訓練とテストデータに分割\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train : (100, 32, 32, 3)\n",
      "y_train : (100, 1, 10)\n",
      "y_label : (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# カラー画像のデータセット\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train_cifar10, y_train_cifar10), (x_test_cifar10, y_test_cifar10) = cifar10.load_data()\n",
    "\n",
    "# プロトタイプなので100データだけ使用する\n",
    "x_train_cifar10 = x_train_cifar10[:100]\n",
    "y_train_cifar10 = y_train_cifar10[:100]\n",
    "y_label_cifar10 = y_train_cifar10\n",
    "y_train_cifar10 = np.identity(10)[y_train_cifar10]\n",
    "del x_test_cifar10, y_test_cifar10\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('x_train : {}'.format(x_train_cifar10.shape))\n",
    "print('y_train : {}'.format(y_train_cifar10.shape))\n",
    "print('y_label : {}'.format(y_label_cifar10.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正答率を算出する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 正答率を算出する\n",
    "# def accuracy_score(X, y, params):\n",
    "#     y_pred =  predict(X, params)\n",
    "#     y_pred_number = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "#     y_label = np.argmax(y, axis=1)\n",
    "    \n",
    "#     data_size = X.shape[0]\n",
    "    \n",
    "#     correct_count = np.sum([y_label == y_pred_number]) \n",
    "#     score = correct_count / data_size * 100\n",
    "    \n",
    "#     return round(score, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### レイヤークラス\n",
    "\n",
    "親クラスとしてlayerクラスを作成。\n",
    "\n",
    "以後、各機能のレイヤークラスはこの親クラスを継承することにする。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, params={}):\n",
    "        if 'input_shape' in params:\n",
    "            self.in_shape = params['input_shape']\n",
    "        else:\n",
    "            self.in_shape = None\n",
    "            \n",
    "        if 'output_shape' in params:\n",
    "            self.out_shape = params['output_shape']\n",
    "        else:\n",
    "            self.out_shape = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最終層\n",
    "\n",
    "分類問題用の最終層クラス（ソフトマックスとクロスエントロピーの結合層）\n",
    "\n",
    "TODO: 正則化項がついていない。　すべての層の重みWを集計するメソッドをNetworkクラスに作成してそれを呼ばなくてはいけないため後回し。\n",
    "\n",
    "回帰問題用に別クラスが必要になる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソフトマックス関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コスト関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, y_pred):\n",
    "    \n",
    "    data_size = y.shape[0]\n",
    "\n",
    "    # クロスエントロピー誤差関数　y_predは０になりえるので -inf にならないためにすごく小さい補正値を入れる\n",
    "    cross_entorpy = -np.sum(y * np.log(y_pred + 1e-7))\n",
    "    \n",
    "    error = cross_entorpy  / data_size\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以前の正則化項つきのクロスエントロピー関数　いずれ使うのでそのままにしておく。\n",
    "# def cost(y, y_pred, params, lam=0.01):\n",
    "#     data_size = y.shape[0]\n",
    "#     #  正則化項\n",
    "#     weight_sum = sum([np.sum(matrix**2) for key, matrix in params.items() if \"W\" in key])\n",
    "#     reg_term = (lam /2) * (weight_sum)\n",
    "#     # クロスエントロピー誤差関数　y_predは０になりえるので -inf にならないためにすごく小さい補正値を入れる\n",
    "#     cross_entorpy = -np.sum(y * np.log(y_pred + 1e-7))\n",
    "    \n",
    "#     cost = (cross_entorpy + reg_term) / data_size\n",
    "#     return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss(Layer):\n",
    "    def __init__(self, params={}):\n",
    "        super(SoftmaxWithLoss, self).__init__(params)\n",
    "        self.loss = None # 損失関数\n",
    "        self.y = None       # softmaxの出力\n",
    "        self.t = None       # 教師データ（one-hot vector)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size # delta3に相当\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### アフィン変換層 (xW + b)\n",
    "\n",
    "更新手法を\n",
    "- sgd\n",
    "- adagrad\n",
    "- adam\n",
    "\n",
    "と切り替えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine(Layer):\n",
    "    \n",
    "    def __init__(self, unit_size=100, params= {}):\n",
    "        super(Affine, self).__init__(params)\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.x = None\n",
    "        self.unit_size = unit_size\n",
    "        # パラメータの微分値\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        # 学習率のセット\n",
    "        if 'lr' in params:\n",
    "            self.lr = params['lr']\n",
    "        else:\n",
    "            self.lr = 0.01\n",
    "\n",
    "        \n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape # N F(特徴数)で来ると想定\n",
    "        N, F = in_shape\n",
    "        \n",
    "        self.out_shape = (N, self.unit_size)\n",
    "        \n",
    "        # 重みの初期化\n",
    "        # im2colを見越して２次元の重みとして実装\n",
    "        self.W = np.random.randn(F, self.unit_size)\n",
    "        self.b = np.zeros([1, self.unit_size])\n",
    "        # TODO 初期化を選択できるように設定する　ひとまずはガウス初期化\n",
    "#         if params['init'] == 'gauss':\n",
    "        self.W *= 0.01\n",
    "#         elif params['init'] == 'xavier':\n",
    "#             # 入力層のユニット数 N * out_h * out_w ? \n",
    "#             self.W /= np.sqrt(self.out_shape[0] * self.out_shape[1] * self.out_shape[2])\n",
    "#         else: # He\n",
    "#             self.W = self.W / np.sqrt(self.out_shape[0] * self.out_shape[1] * self.out_shape[2]) * np.sqrt(2) \n",
    "\n",
    "        # オプティマイザの設定\n",
    "        # optimizeメソッドをoptimizerによって切り替える。\n",
    "        if 'optimizer' in params:\n",
    "            if params['optimizer']=='sgd':\n",
    "                self.optimize = self.update_sgd\n",
    "            elif params['optimizer'] == 'adagrad':\n",
    "                self.h = np.zeros_like(self.W)\n",
    "                self.optimize = self.update_adagrad\n",
    "            else: # params['optimizer'] == 'adam':\n",
    "                self.m = np.zeros_like(self.W)\n",
    "                self.v = np.zeros_like(self.W)\n",
    "                self.beta1 = 0.9\n",
    "                self.beta2 = 0.999\n",
    "                self.optimize = self.update_adam\n",
    "        else:\n",
    "            params['optimizer'] = 'adam'\n",
    "            self.optimize = self.update_adam\n",
    "            \n",
    "        # 次の層の初期化のために出力shapeを返す\n",
    "        print(\"Affine : out {} optimizer {} unit {} \".format(self.out_shape, params['optimizer'], self.unit_size))\n",
    "        return self.out_shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def update_sgd(self):\n",
    "        self.W -= self.lr * self.dW\n",
    "        self.b -= self.lr * self.db\n",
    "    \n",
    "    # adagrad 少しずつ更新量が減っていく\n",
    "    def update_adagrad(self, lr = 0.01):\n",
    "        self.h += self.dW ** 2\n",
    "        self.W -= self.lr * self.dW / (np.sqrt(self.h) + 1e-7)\n",
    "        self.b -= self.lr * self.db\n",
    "        \n",
    "    \n",
    "    def update_adam(self, lr = 0.01):\n",
    "        self.m = self.beta1 * self.m + (1- self.beta1) * self.dW\n",
    "        self.v = self.beta2 * self.v + (1- self.beta2) * (self.dW * self.dW)\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1)\n",
    "        v_hat = self.v / (1 - self.beta2)\n",
    "        \n",
    "        self.W -= self.lr * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "        self.b -= self.lr * self.db\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数層\n",
    "\n",
    "活性化関数層は\n",
    "\n",
    "- ReLU\n",
    "- tanh\n",
    "- シグモイド関数\n",
    "\n",
    "を切り替えて使用できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    '''\n",
    "    活性化関数を設定できる \n",
    "    'tanh'\n",
    "    'sigmoid'\n",
    "    'relu'\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super(Activation, self).__init__(params)\n",
    "        self.out = None\n",
    "        self.mask = None\n",
    "        # optimizeメソッドを\n",
    "        if 'activation' in params:\n",
    "            if params['activation']=='tanh':\n",
    "                self.forward = self.forward_tanh\n",
    "                self.backward = self.backward_tanh\n",
    "            elif params['activation'] == 'sigmoid':\n",
    "                self.forward = self.forward_sigmoid\n",
    "                self.backward = self.backward_sigmoid\n",
    "            else: # params['activation'] == 'relu':\n",
    "                self.forward = self.forward_relu\n",
    "                self.backward = self.backward_relu\n",
    "        else:\n",
    "            params['activation'] = 'relu'\n",
    "            self.forward = self.forward_relu\n",
    "            self.backward = self.backward_relu\n",
    "            \n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape # N H W Cで来ると想定\n",
    "        self.out_shape = self.in_shape\n",
    "            \n",
    "        # 次の層の初期化のために出力shapeを返す\n",
    "        print(\"Activation : out {}   func : {} \".format(self.out_shape, params['activation']))\n",
    "        return self.out_shape\n",
    "     \n",
    "    def forward_relu(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward_relu(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "\n",
    "    # tanh \n",
    "    def forward_tanh(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward_tanh(self, dout):\n",
    "        dx = dout * (1 - np.tanh(dout)**2)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    # sigmoid関数\n",
    "    def forward_sigmoid(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward_sigmoid(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチノーマリゼーション層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    def __init__(self, params):\n",
    "        super(BatchNorm, self).__init__(params)\n",
    "        self.out = None\n",
    "        self.beta = 0.0\n",
    "        self.gamma = 1.0\n",
    "        self.lr = params['lr']\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "        '''\n",
    "        計算式は下記を参照\n",
    "        https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "        '''\n",
    "    \n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape # N H W Cで来ると想定\n",
    "        self.out_shape = self.in_shape\n",
    "            \n",
    "        # 次の層の初期化のために出力shapeを返す\n",
    "        print(\"BatchNorm : out {} lr {}  \".format(self.out_shape, self.lr))\n",
    "        return self.out_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #data_size, input_size = x.shape\n",
    "        \n",
    "        # 単に標準化する\n",
    "        # out = (x - np.mean(x, axis=0)) / np.var()\n",
    "        \n",
    "        # step1: 平均を求める\n",
    "        mu = np.mean(x, axis=0)\n",
    "        \n",
    "        # step2: 偏差\n",
    "        self.xmu = x - mu\n",
    "        \n",
    "        # step3 : 偏差の２乗\n",
    "        sq = self.xmu ** 2\n",
    "        \n",
    "        # step4 : 分散を求める\n",
    "        self.var = np.var(x, axis=0)\n",
    "        \n",
    "        # step5 : 分散のルートを取った値を求める\n",
    "        self.sqrtvar = np.sqrt(self.var + self.eps)\n",
    "        \n",
    "        # step6 : sqrtvarの逆数（invert）\n",
    "        self.ivar = 1.0/ self.sqrtvar\n",
    "        \n",
    "        # step7 : 標準化した値\n",
    "        self.xhat = self.xmu * self.ivar\n",
    "        \n",
    "        # step8\n",
    "        gammax = self.gamma * self.xhat\n",
    "        \n",
    "        # step9\n",
    "        out = gammax + self.beta\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        #get the dimensions of the input/output\n",
    "        N, D = dout.shape\n",
    "        \n",
    "        # step9\n",
    "        self.d_beta = np.sum(dout, axis=0)\n",
    "        dgammax = dout #not necessary, but more understandable\n",
    "        \n",
    "        # step8\n",
    "        self.d_gamma = np.sum(dgammax*self.xhat, axis=0)\n",
    "        dxhat = dgammax * self.gamma\n",
    "        \n",
    "        # step7\n",
    "        divar = np.sum(dxhat*self.xmu, axis=0)\n",
    "        dxmu1 = dxhat * self.ivar\n",
    "        \n",
    "        # step6\n",
    "        dsqrtvar = -1. /(self.sqrtvar**2) * divar\n",
    "        \n",
    "        # step5\n",
    "        dvar = 0.5 * 1. / np.sqrt(self.var+self.eps) * dsqrtvar\n",
    "        \n",
    "        # step4\n",
    "        dsq = 1. / N * np.ones((N, D)) * dvar\n",
    "        \n",
    "        # step3\n",
    "        dxmu2 = 2 * self.xmu * dsq\n",
    "        \n",
    "        # step2\n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "        dmu = -1 * np.sum(dxmu1 + dxmu2, axis=0)\n",
    "        \n",
    "        # step1\n",
    "        dx2 = 1. / N * np.ones((N, D)) * dmu\n",
    "        \n",
    "        # step0\n",
    "        dx = dx1 + dx2\n",
    "        \n",
    "        return dx\n",
    "\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.gamma -= self.lr * self.d_gamma\n",
    "        self.beta -= self.lr * self.d_beta\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ドロップアウト層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, params):\n",
    "        super(Dropout, self).__init__(params)\n",
    "        if 'dropout_ratio' in params:\n",
    "            self.dropout_ratio = params['dropout_ratio']\n",
    "        else:\n",
    "            self.dropout_ratio = 0.5\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg :\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1 - self.dropout_ratio)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 複数層を束ねるクラス(ネットワーククラス)\n",
    "\n",
    "改良したいところ\n",
    "\n",
    "addで層を追加（好きに追加できる。）\n",
    "\n",
    "各層の親クラスを作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "\n",
    "# class Layers:\n",
    "#     def __init__(self, params):\n",
    "#         unit_size_list = [params['input_size']]\n",
    "#         unit_size_list.extend(params['hidden_layer_list'])\n",
    "#         unit_size_list.append(params['output_size'])\n",
    "        \n",
    "#         self.params = {}\n",
    "        \n",
    "#         # レイヤの生成\n",
    "#         self.layers = OrderedDict()\n",
    "        \n",
    "#         # とりあえずここにべた書きで書けるようにする\n",
    "        \n",
    "# #         for i in range(1, len(unit_size_list)):\n",
    "# #             # 重みの初期化\n",
    "# #             init_W = np.random.randn(unit_size_list[i-1], unit_size_list[i])\n",
    "# #             init_b = np.zeros([1, unit_size_list[i]])\n",
    "# #             if params['init'] == 'gauss':\n",
    "# #                 init_W *= 0.01\n",
    "# #             elif params['init'] == 'xavier':\n",
    "# #                 init_W /= np.sqrt(unit_size_list[i-1])\n",
    "# #             else: # He\n",
    "# #                 init_W = init_W / np.sqrt(unit_size_list[i-1]) * np.sqrt(2) \n",
    "                \n",
    "# #             # アフィン変換層（Wx + b）を追加する\n",
    "# #             self.layers['Affine' + str(i)] = Affine(init_W, init_b, params)\n",
    "            \n",
    "# #             # 最終層以外はバッチノーマリゼーション層と活性化関数層を追加する\n",
    "# #             if i < (len(unit_size_list)-1):\n",
    "# #                 if params['batch_norm'] == True:\n",
    "# #                     self.layers['BatchNorm' + str(i)] = BatchNorm(params)\n",
    "# #                 self.layers['Active' + str(i)] = Activation(params)\n",
    "# # #                 if params['dropout_ratio'] > 0:\n",
    "# # #                     self.layers['Dropout' + str(i)] = Dropout(params)\n",
    "        \n",
    "#         self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "#         # self.params['hidden_layer_num'] = len(unit_size_list)-1\n",
    "        \n",
    "#     def predict(self, x):\n",
    "#         # forwardを繰り返す\n",
    "#         # ソフトマックスを通さなくても答えは出るのでこれで予測とする \n",
    "#         # argmaxでラベルを取れる\n",
    "#         for layer in self.layers.values():\n",
    "#             x =layer.forward(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def accuracy(self, x, t):\n",
    "#         # 正答率を小数点第二桁で出力する\n",
    "#         y_pred = self.predict(x)\n",
    "#         y_pred = np.argmax(y_pred, axis=1)\n",
    "#         y_true = np.argmax(t, axis=1)\n",
    "#         data_size = x.shape[0]\n",
    "\n",
    "#         correct_count = np.sum([y_true == y_pred]) \n",
    "#         score = correct_count / data_size * 100\n",
    "\n",
    "#         return round(score, 2)\n",
    "    \n",
    "#     def loss(self, x, t):\n",
    "#         y = self.predict(x)\n",
    "        \n",
    "#         return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    \n",
    "#     def optimize(self, x, t):\n",
    "        \n",
    "#         # forward \n",
    "#         self.loss(x, t)\n",
    "        \n",
    "#         # backward\n",
    "#         dout = self.lastLayer.backward(1)\n",
    "#         layers = list(self.layers.values())\n",
    "#         layers.reverse()\n",
    "#         for layer in layers:\n",
    "#             dout = layer.backward(dout)\n",
    "            \n",
    "#         # optimizeメソッドがある層は更新を行う\n",
    "#         # AffineとBatchNorm層のみ行うはず\n",
    "#         for layer in self.layers.values():\n",
    "#             if hasattr(layer, \"optimize\"):\n",
    "#                 layer.optimize()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO : Gradient Checkを導入する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# class DNN:\n",
    "#     def __init__(self, init='gauss', iteration = 500, lr = 0.05,  lam = 0.01, \n",
    "#                  batch_mode = 'mini', activation='relu',\n",
    "#                  batch_size_rate = 0.1, hidden_layer_list = [5], optimizer='sgd',\n",
    "#                  batch_norm=False, dropout_ratio=0.0):\n",
    "#         \"\"\" ハイパーパラメータ解説\n",
    "#         init: 初期化方法\n",
    "#             'he' : \n",
    "#             'gauss' \n",
    "#             'xavier'\n",
    "#         lr : 学習率\n",
    "#         lam : 正則化項の率\n",
    "#         batch_size: バッチサイズ\n",
    "#             'batch' : フルサイズ\n",
    "#             'mini' 0< x< 1: フルサイズ割合 0.1なら全体の0.1サイズ使用する\n",
    "#             'online' : オンライン学習　１データのみ\n",
    "#         hidden_layer_list : 隠れ層のリスト、層のユニットをリストで入力　例[2, 3]　ユニット数２、ユニット数３の隠れ層\n",
    "#         optimizer : 勾配の更新手法\n",
    "#             'sgd' : 確率的勾配降下法\n",
    "#             'adam': \n",
    "#             'adagrad':\n",
    "#         activation: 活性化関数の名前\n",
    "#             'relu' : ReLU関数\n",
    "#             'tanh' : tanh\n",
    "#             'sigmoid' : シグモイド関数\n",
    "#         \"\"\"\n",
    "#         self.params = {}\n",
    "#         self.params['iteration'] = iteration\n",
    "#         self.params['init'] = init\n",
    "#         self.params['lr'] = lr\n",
    "#         self.params['lam'] = lam # 正則化項用の係数　今は使っていない\n",
    "#         self.params['batch_mode'] = batch_mode # データ数が決まったらそれに基づいて変更する\n",
    "#         self.params['batch_size_rate'] = batch_size_rate # ミニバッチ法のときのみ使用する\n",
    "#         self.params['hidden_layer_list'] = hidden_layer_list\n",
    "#         self.params['optimizer'] = optimizer\n",
    "#         self.params['batch_norm'] = batch_norm\n",
    "#         self.params['dropout_ratio'] = dropout_ratio\n",
    "#         self.params['activation'] = activation # 活性化関数\n",
    "        \n",
    "#     def train(self, X, y, params={}):\n",
    "#         # 入力パラメータがあれば更新する\n",
    "#         for key in params:\n",
    "#                 self.params[key] = params[key]\n",
    "        \n",
    "#         # 正規化　必要？\n",
    "#         X = X / 255.0\n",
    "        \n",
    "#         # 訓練とテストデータに分割\n",
    "#         X_train, X_test, y_train, y_test = \\\n",
    "#             train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#         self.params['data_size'] = X_train.shape[0]\n",
    "#         self.params['input_size'] = X_train.shape[1]\n",
    "#         self.params['output_size'] = y_train.shape[1]\n",
    "\n",
    "        \n",
    "#         # コストや正答率の学習曲線を引くためのリストを用意\n",
    "#         past_train_costs = []\n",
    "#         past_test_costs = []\n",
    "#         past_train_accuracy = []\n",
    "#         past_test_accuracy = []\n",
    "        \n",
    "#         # 初期化\n",
    "#         # 重み初期化\n",
    "#         # バッチサイズの設定\n",
    "#         if self.params['batch_mode'] == 'batch':\n",
    "#             self.params['batch_size'] = self.params['data_size']\n",
    "#         elif self.params['batch_mode'] == 'mini':\n",
    "#             self.params['batch_size'] = int(self.params['data_size']  * self.params['batch_size_rate'] ) \n",
    "#         else:\n",
    "#             self.params['batch_size'] = 1\n",
    "#         # 隠れ層やレイヤーインスタンス生成\n",
    "#         self.params['layer'] = Layers(self.params)\n",
    "        \n",
    "        \n",
    "#         # 何イテレーションで1エポックか\n",
    "#         epoch_per_i = int(self.params['data_size'] / self.params['batch_size'])\n",
    "        \n",
    "#         ##################\n",
    "#         # 最急降下法での学習\n",
    "#         ##################\n",
    "#         for i in range(self.params['iteration']):\n",
    "            \n",
    "#             # 学習に使用するデータをサンプリング\n",
    "#             choice_index = np.random.choice(self.params['data_size'], self.params['batch_size'])\n",
    "#             X_batch, y_batch = X_train[choice_index], y_train[choice_index]\n",
    "            \n",
    "#             # 誤差逆伝播法によって勾配を求め、値を更新\n",
    "#             self.params['layer'].optimize(X_batch, y_batch)\n",
    "            \n",
    "#             # 1エポックごとに正答率とコストを算出して保存する\n",
    "#             if i % epoch_per_i == 0:              \n",
    "#                 past_train_accuracy.append(self.params['layer'].accuracy(X_train, y_train))\n",
    "#                 past_test_accuracy.append(self.params['layer'].accuracy(X_test, y_test))\n",
    "                \n",
    "#                 past_train_costs.append(self.params['layer'].loss(X_train, y_train))\n",
    "#                 past_test_costs.append(self.params['layer'].loss(X_test, y_test))\n",
    "            \n",
    "#         return past_train_accuracy, past_test_accuracy, past_train_costs, past_test_costs \n",
    "    \n",
    "\n",
    "        \n",
    "#     # 現在のパラメータで予測値を確率かラベルで出力する。\n",
    "#     def predict(self, X, probability=False):\n",
    "#         predict = self.params['layer'].predict(X, train_flg=False)\n",
    "#         predict_proba = softmax(predict)\n",
    "#         if probability== True:\n",
    "#             return predict_proba\n",
    "#         else:\n",
    "#             return np.argmax(predict_proba, axis=1)\n",
    "        \n",
    "#     def plot_learning_curve(self, X, y, metrics='acc', params={}): \n",
    "#         past_train_accuracy, past_test_accuracy, past_train_costs, past_test_costs = self.train(X, y, params)\n",
    "#         plt.figure(figsize=(6,4))\n",
    "#         # count_epoch = self.params['iteration'] // self.params['data_size'] + 1\n",
    "#         if metrics == 'cost':\n",
    "#             plt.plot(past_train_costs, color='orange', label='train')\n",
    "#             plt.plot(past_test_costs, color='lime', label='test')\n",
    "#             plt.ylabel(\"cost\", fontsize=15)\n",
    "#             print(\"last train cost is {}\".format(past_train_costs[-1]))\n",
    "#             print(\"last test cost is {}\".format(past_test_costs[-1]))\n",
    "#         else:\n",
    "#             #plt.plot(np.array(past_train_accuracy), color='r')\n",
    "#             plt.plot(past_train_accuracy, color='orange', label='train')\n",
    "#             plt.plot(past_test_accuracy, color='lime', label='test')\n",
    "#             plt.ylabel(\"accuracy\", fontsize=15)\n",
    "#             print(\"last train accuracy is {}\".format(past_train_accuracy[-1]))\n",
    "#             print(\"last test accuracy is {}\".format(past_test_accuracy[-1]))\n",
    "#             plt.ylim(-0.5, 100.5)\n",
    "\n",
    "#         plt.legend()\n",
    "#         plt.title('Learning Curve', fontsize=20)\n",
    "#         plt.xlabel(\"iteration[epoch]\", fontsize=15)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プロットはクラスに入れず、trainの返り値を使ってplotすればよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DNN(iteration=10, optimizer='adam', \n",
    "#             hidden_layer_list = [100], batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'optimizer': 'adam',\n",
    "#          'batch_mode' : 'mini',\n",
    "#          'init': 'he',\n",
    "#          'lr': 0.007}\n",
    "\n",
    "# #past_train_accuracy, past_test_accuracy = model.train(X,y, params)\n",
    "# # model.plot_learning_curve(X,y, 'cost', params)\n",
    "\n",
    "# model.plot_learning_curve(x_train,y_train, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提出用ファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コメントアウトを外せば提出ファイルができる。\n",
    "# test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# X_sub = test_df\n",
    "# X_sub = np.array(X_sub)\n",
    "\n",
    "# # train_flgでエラー中　すべてparamsに押し込めれば良い\n",
    "# Y_pred = model.predict(X_sub)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#        \"ImageId\": np.array(test_df.index) + 1,\n",
    "#        \"Label\": Y_pred\n",
    "#    })\n",
    "\n",
    "# submission.to_csv(\"./submission_001.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入出力サイズ対応が必要か\n",
    "\n",
    "### DNNクラス\n",
    "\n",
    "### Layersクラス\n",
    "\n",
    "### 単層クラス\n",
    "\n",
    "- Affine層\n",
    "    - SGD\n",
    "    - Adam\n",
    "    - adgrad\n",
    "- 活性化関数\n",
    "    - tanh\n",
    "    - ReLU\n",
    "    - sigmoid\n",
    "- バッチノーマリゼーション層\n",
    "- ドロップアウト層(要修正)\n",
    "\n",
    "\n",
    "- 畳み込み層　（forward確認中）\n",
    "- プーリング層 （forward、　）\n",
    "- Flatten層 (済)\n",
    "\n",
    "im2col col2im\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['activation'] = 'relu'\n",
    "act1 = Activation(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 28, 28)\n",
      "(80, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(act1.forward(x_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 32, 32, 3)\n",
      "(100, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_cifar10.shape)\n",
    "print(act1.forward(x_train_cifar10).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数はサイズを気にしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cifar10[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattenクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "    # (N, H, W, C)のデータを\n",
    "    # (N, H * W * C)に変更する\n",
    "    def __init__(self, params={}):\n",
    "        super(Flatten, self).__init__(params)\n",
    "        # 入出力サイズ以外のパラメータはない\n",
    "        \n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape # N H W Cで来ると想定\n",
    "        N, H, W, C = in_shape\n",
    "        self.out_shape = (N, H * W *C) \n",
    "            \n",
    "        # 次の層の初期化のために出力shapeを返す\n",
    "        print(\"Flatten : out {} \".format(self.out_shape))\n",
    "        return self.out_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input_shape = x.shape\n",
    "        out = np.array([elem.flatten() for elem in x])\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.reshape(self.input_shape)\n",
    "        \n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 28, 28, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:, :, :, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat = Flatten()\n",
    "forward_val = flat.forward(x_train[:, :, :, np.newaxis])\n",
    "forward_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 28, 28, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat.backward(forward_val).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力が変わらないパディング\n",
    "\n",
    "フィルターサイズが奇数であることを条件に、\n",
    "フィルターサイズを２で割った商をパディング数とすればよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "(100, 34, 34, 3)\n"
     ]
    }
   ],
   "source": [
    "filter_size = 3\n",
    "\n",
    "pad = filter_size // 2\n",
    "\n",
    "print(x_train_cifar10[0].shape)\n",
    "      \n",
    "print(np.pad(x_train_cifar10,[(0,0),(pad, pad),(pad, pad),(0,0)],'constant').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 同じように'edge'にすればパディングは隣接値をそのまま使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 34, 34, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.pad(x_train_cifar10,[(0,0),(pad, pad),(pad, pad),(0,0)],'edge').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### im2colの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### これをim2colの反対のcol2imで元のデータ形状に戻してやれば良い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(x, filter_size=4, stride=4, pad=0, padding='constant'):\n",
    "    # 4次元データを2次元データに変形する\n",
    "    # XXX: もしかしたらできているかも？？　今の所パディングして数が変わるとエラーになる\n",
    "    # TODO: 上記を修正して、畳み込み層にも使えるように修正する\n",
    "    \n",
    "    # パディングして外堀を埋める\n",
    "    if pad > 0: \n",
    "        x = np.pad(x,[(0,0),(pad, pad),(pad, pad),(0,0)], padding)\n",
    "\n",
    "    # データ数　高さ　幅　チャネルを取得\n",
    "    N, H, W, C = x.shape\n",
    "    \n",
    "    # それぞれのブロックごとにflatten\n",
    "    return np.array([x[n, h:h+filter_size, w:w+filter_size, c].flatten() for c in range(C) for n in range(N) \\\n",
    "            for h in range(0, H-filter_size+1, stride) for w in range(0, W-filter_size+1, stride)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im2colの逆をやる関数\n",
    "def col2im(x, input_shape, filter_size=4, stride=4, pad=0, padding='constant'):\n",
    "    \n",
    "    # 戻すデータの形状\n",
    "    N, H, W, C = input_shape\n",
    "    \n",
    "    # リターン箱を作る\n",
    "    img = np.zeros(input_shape)\n",
    "    # ブロック数\n",
    "    block_num = (pad * 2 + W - filter_size)// stride + 1 # 本当は高さ　幅別々に計算する\n",
    "\n",
    "    # １チャネルに何行あるか\n",
    "    c_vol = int(x.shape[0] / C)\n",
    "    # c_vol = int(ret.shape[0] / C)\n",
    "    n_vol = int(c_vol / N)\n",
    "\n",
    "    for i, line in enumerate(x):\n",
    "        # 帯からブロックにする\n",
    "        block = line.reshape(filter_size, filter_size)\n",
    "\n",
    "        channel_i = i//c_vol\n",
    "\n",
    "        data_i = i % c_vol // n_vol\n",
    "\n",
    "        start_h, start_w = divmod(i % n_vol, block_num) # 本当はblock_h\n",
    "        start_h, start_w = int(start_h), int(start_w)\n",
    "\n",
    "        end_h = start_h +  filter_size# filter_size or filter_height\n",
    "        end_w = start_w + filter_size # filter_size or filter_weight\n",
    "#         print(block.shape)\n",
    "#         print(img[data_i, start_h:end_h, start_w:end_w, channel_i].shape)\n",
    "        #print(\"{} {} {} {} \".format(data_i, start_h, start_w, channel_i))\n",
    "        img[data_i, start_h:end_h, start_w:end_w, channel_i] = block\n",
    "        \n",
    "    return img[:, pad:pad+H, pad:pad+W, :]                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = im2col(x_train_cifar10[:10], 3, 1, 0)\n",
    "x_train_cifar10[:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27000, 9)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col2im(ret, x_train_cifar10[:10].shape, 3, 1, 0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### col2im im2colが完成！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPoolingクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(Layer):\n",
    "    \n",
    "    def __init__(self, pool_size=4, stride=-1, pad=0, params={}):\n",
    "        super(MaxPooling, self).__init__(params)\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.stride = self.pool_size if self.stride == -1 else stride\n",
    "        # パディング方法をしてできたほうがよい？　今はゼロ埋め固定\n",
    "        # 最大値の場所を保存して、backwardでその場所以外は伝播しない\n",
    "        self.max_index = None\n",
    "        self.input_shape = None # 親クラスで定義するのでここには後々必要なくなる\n",
    "        \n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape # N H W Cで来ると想定\n",
    "        N, H, W, C = in_shape\n",
    "        # 出力の高さと幅を計算する\n",
    "        out_h = 1 + int((H + 2*self.pad - self.pool_size) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - self.pool_size) / self.stride)\n",
    "        \n",
    "        self.out_shape = (N, out_h, out_w, C)\n",
    "            \n",
    "        # 次の層の初期化のために出力shapeを返す\n",
    "        print(\"Pooling : out {} filter {} stride {} pad {} \".format(self.out_shape, self.pool_size, self.stride, self.pad))\n",
    "        return self.out_shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.input_shape = x.shape\n",
    "        N, H, W, C = x.shape\n",
    "        \n",
    "        col = im2col(x, self.pool_size, self.stride, self.pad)\n",
    "        col_max = np.max(col, axis=1)\n",
    "        self.max_index = np.argmax(col, axis=1)\n",
    "        \n",
    "        # 出力サイズを確認\n",
    "        out_h = (H + 2*self.pad - self.pool_size)// self.stride + 1 \n",
    "        out_w = (W + 2*self.pad - self.pool_size)// self.stride + 1 \n",
    "        \n",
    "        # 整形\n",
    "        return col_max.reshape(C, N, out_h, out_w).transpose(1, 2, 3, 0)\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "\n",
    "        filter_size = dout.shape[1]\n",
    "        dout_line = dout.transpose(3, 0, 1, 2).reshape(-1)\n",
    "        \n",
    "        # 返り値の箱を作る\n",
    "        ret = np.zeros([dout_line.shape[0], self.pool_size* self.pool_size])\n",
    "        \n",
    "        # 最大値の場所にdoutを流し込む\n",
    "        for i, max_i in enumerate(poollayer.max_index):\n",
    "            ret[i, max_i] = dout_line[i]\n",
    "            \n",
    "        # 元の形状に戻してリターン\n",
    "        return col2im(ret, self.input_shape, self.pool_size, self.stride, self.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(Layer):\n",
    "    \n",
    "    def __init__(self, out_channel = 1, filter_size=3, stride=1, pad=0, bias=True, params={}):\n",
    "        # biasなしに対応していない。 dbを更新しなければよい？\n",
    "        self.out_channel =out_channel\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.bias = bias\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        ##### Affineからパクリ ########\n",
    "        self.x = None\n",
    "        # パラメータの微分値\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.optimize = None\n",
    "        # 学習率のセット\n",
    "        if 'lr' in params:\n",
    "            self.lr = params['lr']\n",
    "        else:\n",
    "            self.lr = 0.01\n",
    "        self.x_2dim = None # im2col後のxの値を保持\n",
    "        \n",
    "        \n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape # N H W Cで来ると想定\n",
    "        N, H, W, C = in_shape\n",
    "        # 出力の高さと幅を計算する\n",
    "        out_h = 1 + int((H + 2*self.pad - self.filter_size) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - self.filter_size) / self.stride)\n",
    "        \n",
    "        self.out_shape = (N, out_h, out_w, self.out_channel)\n",
    "        \n",
    "        # 重みの初期化\n",
    "        # im2colを見越して２次元の重みとして実装\n",
    "        self.W = np.random.randn(self.filter_size * self.filter_size * self.in_shape[3], self.out_channel)\n",
    "        self.b = np.zeros([self.out_channel, 1])\n",
    "        # TODO 初期化を選択できるように設定する　ひとまずはガウス初期化\n",
    "#         if params['init'] == 'gauss':\n",
    "        self.W *= 0.01\n",
    "#         elif params['init'] == 'xavier':\n",
    "#             # 入力層のユニット数 N * out_h * out_w ? \n",
    "#             self.W /= np.sqrt(self.out_shape[0] * self.out_shape[1] * self.out_shape[2])\n",
    "#         else: # He\n",
    "#             self.W = self.W / np.sqrt(self.out_shape[0] * self.out_shape[1] * self.out_shape[2]) * np.sqrt(2) \n",
    "        \n",
    "        # 更新式のスイッチング\n",
    "        # optimizeメソッドをoptimizerによって切り替える。\n",
    "        if 'optimizer' in params:\n",
    "            if params['optimizer']=='sgd':\n",
    "                self.optimize = self.update_sgd\n",
    "            elif params['optimizer'] == 'adagrad':\n",
    "                self.h = np.zeros_like(W)\n",
    "                self.optimize = self.update_adagrad\n",
    "            else: # params['optimizer'] == 'adam':\n",
    "                self.m = np.zeros_like(W)\n",
    "                self.v = np.zeros_like(W)\n",
    "                self.beta1 = 0.9\n",
    "                self.beta2 = 0.999\n",
    "                self.optimize = self.update_adam\n",
    "        else:\n",
    "            self.optimize = self.update_sgd\n",
    "            \n",
    "        # 次の層の初期化のために出力shapeを返す\n",
    "        print(\"Conv : out {} filter {} stride {} pad {} \".format(self.out_shape, self.filter_size, self.stride, self.pad))\n",
    "        return self.out_shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_C = self.in_shape[3]\n",
    "        out_C = self.out_shape[3]\n",
    "        N = self.in_shape[0]\n",
    "        \n",
    "        # 出力の高さと幅\n",
    "        out_h, out_w = self.out_shape[1], self.out_shape[2]\n",
    "\n",
    "        # ２次元配列に変換する\n",
    "        # (out_h * out_w * N * C, filter_size*filter_size)\n",
    "        print(\"x.shape {}\".format(x.shape))\n",
    "        x_2dim = im2col(x, self.filter_size, self.stride, self.pad)\n",
    "        print(\"x_2dim.shape {}\".format(x_2dim.shape))\n",
    "        print(\"filter_size {} stride {} pad {}\".format(self.filter_size, self.stride, self.pad))\n",
    "        self.x_2dim = x_2dim\n",
    "        \n",
    "        # 各色（チャネル）ごとに重みを分割\n",
    "        W_color = self.W.reshape(in_C, -1, out_C)\n",
    "        print(\"in_C {} out_h {} out_w {} N {} \".format(in_C, out_h, out_w, N))\n",
    "        x_2dim_color = x_2dim.reshape(in_C, -1, out_h * out_w * N)\n",
    "        \n",
    "        # 入力チャネルごとに対応するフィルターで計算する\n",
    "        out = np.zeros((in_C, out_C, out_h * out_w * N))\n",
    "        for c in range(in_C):\n",
    "            out[c] = np.dot(W_color[c].T, x_2dim_color[c])\n",
    "        \n",
    "        # 入力チャネルごとの結果を合計する\n",
    "        out = np.sum(out, axis=0)\n",
    "        \n",
    "        out = out + self.b\n",
    "        \n",
    "        # 整形する　out_C N H W\n",
    "        return  out.reshape(out_C, N, out_h, out_w).transpose(1, 2 ,3, 0)\n",
    "        \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        out_c = self.out_shape[3]\n",
    "        dout = dout.reshape(-1, out_c)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)[:, np.newaxis]\n",
    "\n",
    "        # 入力チャネルの数\n",
    "        in_C = self.in_shape[3]\n",
    "        # sum前の状態に戻す　（入力チャネル分複製する）\n",
    "        dout_before_sum = np.tile(dout, (in_C, 1))\n",
    "        self.dW = np.dot(self.x_2dim.T, dout_before_sum)\n",
    "\n",
    "        dout = np.dot(dout, self.W.T)\n",
    "\n",
    "        # (R G B)と横並びなので\n",
    "        #  R\n",
    "        #  G\n",
    "        #  B　と縦方向に変換する\n",
    "        dout = np.vstack(np.split(dout, in_C, axis=1))\n",
    "\n",
    "        dout = col2im(dout, self.in_shape, self.filter_size, self.stride, self.pad)\n",
    "        return dout\n",
    "    \n",
    "    #####affineからパクリ\n",
    "    def update_sgd(self):\n",
    "        self.W -= self.lr * self.dW\n",
    "        self.b -= self.lr * self.db\n",
    "    \n",
    "    \n",
    "    # adagrad 少しずつ更新量が減っていく\n",
    "    def update_adagrad(self, lr = 0.01):\n",
    "        self.h += self.dW ** 2\n",
    "        self.W -= self.lr * self.dW / (np.sqrt(self.h) + 1e-7)\n",
    "        self.b -= self.lr * self.db\n",
    "        \n",
    "    \n",
    "    def update_adam(self, lr = 0.01):\n",
    "        self.m = self.beta1 * self.m + (1- self.beta1) * self.dW\n",
    "        self.v = self.beta2 * self.v + (1- self.beta2) * (self.dW * self.dW)\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1)\n",
    "        v_hat = self.v / (1 - self.beta2)\n",
    "        \n",
    "        self.W -= self.lr * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "        self.b -= self.lr * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolutionクラスの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3)\n",
      "Conv : out (10, 30, 30, 2) filter 3 stride 1 pad 0 \n",
      "shape(10, 30, 30, 2)\n",
      "(10, 30, 30, 2)\n"
     ]
    }
   ],
   "source": [
    "conv = Convolution(2, 3, 1, 0)\n",
    "data = x_train_cifar10[:10]\n",
    "print(data.shape)\n",
    "conv.initialize(data.shape)\n",
    "out = conv.forward(data)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32, 3)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dout_check = conv.backward(out)\n",
    "dout_check.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poolingクラスの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3)\n",
      "shape(10, 10, 10, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 3)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poollayer = MaxPooling(pool_size=3,stride=3, pad=0)\n",
    "print(x_train_cifar10[:10].shape)\n",
    "dout = poollayer.forward(x_train_cifar10[:10])\n",
    "dout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32, 3)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = poollayer.backward(dout)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)            (None, 28, 28, 2)         20        \n",
    "_________________________________________________________________\n",
    "batch_normalization_1 (Batch (None, 28, 28, 2)         8         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 2)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 14, 14, 2)         38        \n",
    "_________________________________________________________________\n",
    "batch_normalization_2 (Batch (None, 14, 14, 2)         8         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 2)           0         \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 7, 7, 2)           38        \n",
    "_________________________________________________________________\n",
    "batch_normalization_3 (Batch (None, 7, 7, 2)           8         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 2)           0         \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 18)                0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 128)               2432      \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 128)               16512     \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 10)                1290  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class LeNetLayers:\n",
    "    def __init__(self, params):\n",
    "        unit_size_list = [params['input_size']]\n",
    "        unit_size_list.extend(params['hidden_layer_list'])\n",
    "        unit_size_list.append(params['output_size'])\n",
    "        \n",
    "        self.params = {}\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        # とりあえずここにべた書きで書けるようにする\n",
    "        self.layers['Conv1'] = Convolution(6, 5, 1, 2)# 2 (3,3) (1,1) same\n",
    "        # self.layers['BatchNorm1'] = BatchNorm(params)\n",
    "        self.layers['Active1'] = Activation(params) # relu\n",
    "        self.layers['Pool1'] = MaxPooling(2, 2, 0, params) \n",
    "        \n",
    "        self.layers['Conv2'] = Convolution(16, 5, 1, 2)# 2 (3,3) (1,1) same\n",
    "        # self.layers['BatchNorm2'] = BatchNorm(params)\n",
    "        self.layers['Active2'] = Activation(params) # relu\n",
    "        self.layers['Pool2'] = MaxPooling(2, 2, 0, params) \n",
    "        \n",
    "        self.layers['Flatten'] = Flatten(params) \n",
    "        \n",
    "        # アフィン変換層（Wx + b）を追加する\n",
    "        self.layers['Affine1'] = Affine(120, params)\n",
    "        self.layers['Active3'] = Activation(params) # relu\n",
    "        self.layers['Affine2'] = Affine(84, params)\n",
    "        self.layers['Active4'] = Activation(params) # relu\n",
    "        self.layers['Affine3'] = Affine(params['output_size'], params)\n",
    "\n",
    "        \n",
    "    def initialize(self, x, y, params):\n",
    "        in_shape = x.shape # N, H, W, C\n",
    "        print(x.shape)\n",
    "        out_shape = y.shape # N, Class\n",
    "        print('###########' * 3)\n",
    "        for i, layer in enumerate(self.layers.values()):\n",
    "            print(\" Layer {}\".format(i))\n",
    "            in_shape = layer.initialize(in_shape, params)\n",
    "            print('###########' * 3)\n",
    "        \n",
    "        \n",
    "#         for i in range(1, len(unit_size_list)):\n",
    "#             # 重みの初期化\n",
    "#             init_W = np.random.randn(unit_size_list[i-1], unit_size_list[i])\n",
    "#             init_b = np.zeros([1, unit_size_list[i]])\n",
    "#             if params['init'] == 'gauss':\n",
    "#                 init_W *= 0.01\n",
    "#             elif params['init'] == 'xavier':\n",
    "#                 init_W /= np.sqrt(unit_size_list[i-1])\n",
    "#             else: # He\n",
    "#                 init_W = init_W / np.sqrt(unit_size_list[i-1]) * np.sqrt(2) \n",
    "                \n",
    "#             # アフィン変換層（Wx + b）を追加する\n",
    "#             self.layers['Affine' + str(i)] = Affine(init_W, init_b, params)\n",
    "            \n",
    "#             # 最終層以外はバッチノーマリゼーション層と活性化関数層を追加する\n",
    "#             if i < (len(unit_size_list)-1):\n",
    "#                 if params['batch_norm'] == True:\n",
    "#                     self.layers['BatchNorm' + str(i)] = BatchNorm(params)\n",
    "#                 self.layers['Active' + str(i)] = Activation(params)\n",
    "# #                 if params['dropout_ratio'] > 0:\n",
    "# #                     self.layers['Dropout' + str(i)] = Dropout(params)\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "        # self.params['hidden_layer_num'] = len(unit_size_list)-1\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # forwardを繰り返す\n",
    "        # ソフトマックスを通さなくても答えは出るのでこれで予測とする \n",
    "        # argmaxでラベルを取れる\n",
    "        for layer in self.layers.values():\n",
    "            x =layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        # 正答率を小数点第二桁で出力する\n",
    "        y_pred = self.predict(x)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(t, axis=1)\n",
    "        data_size = x.shape[0]\n",
    "\n",
    "        correct_count = np.sum([y_true == y_pred]) \n",
    "        score = correct_count / data_size * 100\n",
    "\n",
    "        return round(score, 2)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    \n",
    "    def optimize(self, x, t):\n",
    "        \n",
    "        # forward \n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.lastLayer.backward(1)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # optimizeメソッドがある層は更新を行う\n",
    "        # AffineとBatchNorm層のみ行うはず\n",
    "        for layer in self.layers.values():\n",
    "            if hasattr(layer, \"optimize\"):\n",
    "                layer.optimize()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, init='gauss', iteration = 500, lr = 0.05,  lam = 0.01, \n",
    "                 batch_mode = 'mini', activation='relu',\n",
    "                 batch_size_rate = 0.1, hidden_layer_list = [5], optimizer='sgd',\n",
    "                 batch_norm=False, dropout_ratio=0.0):\n",
    "        \"\"\" ハイパーパラメータ解説\n",
    "        init: 初期化方法\n",
    "            'he' : \n",
    "            'gauss' \n",
    "            'xavier'\n",
    "        lr : 学習率\n",
    "        lam : 正則化項の率\n",
    "        batch_size: バッチサイズ\n",
    "            'batch' : フルサイズ\n",
    "            'mini' 0< x< 1: フルサイズ割合 0.1なら全体の0.1サイズ使用する\n",
    "            'online' : オンライン学習　１データのみ\n",
    "        hidden_layer_list : 隠れ層のリスト、層のユニットをリストで入力　例[2, 3]　ユニット数２、ユニット数３の隠れ層\n",
    "        optimizer : 勾配の更新手法\n",
    "            'sgd' : 確率的勾配降下法\n",
    "            'adam': \n",
    "            'adagrad':\n",
    "        activation: 活性化関数の名前\n",
    "            'relu' : ReLU関数\n",
    "            'tanh' : tanh\n",
    "            'sigmoid' : シグモイド関数\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['iteration'] = iteration\n",
    "        self.params['init'] = init\n",
    "        self.params['lr'] = lr\n",
    "        self.params['lam'] = lam # 正則化項用の係数　今は使っていない\n",
    "        self.params['batch_mode'] = batch_mode # データ数が決まったらそれに基づいて変更する\n",
    "        self.params['batch_size_rate'] = batch_size_rate # ミニバッチ法のときのみ使用する\n",
    "        self.params['hidden_layer_list'] = hidden_layer_list\n",
    "        self.params['optimizer'] = optimizer\n",
    "        self.params['batch_norm'] = batch_norm\n",
    "        self.params['dropout_ratio'] = dropout_ratio\n",
    "        self.params['activation'] = activation # 活性化関数\n",
    "        \n",
    "    def train(self, X, y, params={}):\n",
    "        # 入力パラメータがあれば更新する\n",
    "        for key in params:\n",
    "                self.params[key] = params[key]\n",
    "        \n",
    "        # 正規化　必要？\n",
    "        X = X / 255.0\n",
    "        \n",
    "        # 訓練とテストデータに分割\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        self.params['data_size'] = X_train.shape[0]\n",
    "        self.params['input_size'] = X_train.shape[1]\n",
    "        self.params['output_size'] = y_train.shape[1]\n",
    "\n",
    "        \n",
    "        # コストや正答率の学習曲線を引くためのリストを用意\n",
    "        past_train_costs = []\n",
    "        past_test_costs = []\n",
    "        past_train_accuracy = []\n",
    "        past_test_accuracy = []\n",
    "        \n",
    "\n",
    "        # バッチサイズの設定\n",
    "        if self.params['batch_mode'] == 'batch':\n",
    "            self.params['batch_size'] = self.params['data_size']\n",
    "        elif self.params['batch_mode'] == 'mini':\n",
    "            self.params['batch_size'] = int(self.params['data_size']  * self.params['batch_size_rate'] ) \n",
    "        else:\n",
    "            self.params['batch_size'] = 1\n",
    "        # 隠れ層やレイヤーインスタンス生成\n",
    "        self.params['layer'] = LeNetLayers(self.params)\n",
    "        # 入出力サイズ\n",
    "        print(\"before layer making {}\".format(X_train.shape))\n",
    "        self.params['layer'].initialize(X_train, y_train, self.params)\n",
    "        \n",
    "        # print(self.params['layer'].predict(X_train).shape)\n",
    "        # 確認のためここでブレークする\n",
    "#         if True:\n",
    "#             return self.params['layer'].predict(X_train)\n",
    "        \n",
    "        # 何イテレーションで1エポックか\n",
    "        epoch_per_i = int(self.params['data_size'] / self.params['batch_size'])\n",
    "        \n",
    "        ##################\n",
    "        # 最急降下法での学習\n",
    "        ##################\n",
    "        for i in range(self.params['iteration']):\n",
    "            \n",
    "            # 学習に使用するデータをサンプリング\n",
    "            choice_index = np.random.choice(self.params['data_size'], self.params['batch_size'])\n",
    "            X_batch, y_batch = X_train[choice_index], y_train[choice_index]\n",
    "            \n",
    "            # 誤差逆伝播法によって勾配を求め、値を更新\n",
    "            self.params['layer'].optimize(X_batch, y_batch)\n",
    "            \n",
    "            # 1エポックごとに正答率とコストを算出して保存する\n",
    "            if i % epoch_per_i == 0:              \n",
    "                past_train_accuracy.append(self.params['layer'].accuracy(X_train, y_train))\n",
    "                past_test_accuracy.append(self.params['layer'].accuracy(X_test, y_test))\n",
    "                \n",
    "                past_train_costs.append(self.params['layer'].loss(X_train, y_train))\n",
    "                past_test_costs.append(self.params['layer'].loss(X_test, y_test))\n",
    "            \n",
    "        return past_train_accuracy, past_test_accuracy, past_train_costs, past_test_costs \n",
    "    \n",
    "\n",
    "        \n",
    "    # 現在のパラメータで予測値を確率かラベルで出力する。\n",
    "    def predict(self, X, probability=False):\n",
    "        predict = self.params['layer'].predict(X, train_flg=False)\n",
    "        predict_proba = softmax(predict)\n",
    "        if probability== True:\n",
    "            return predict_proba\n",
    "        else:\n",
    "            return np.argmax(predict_proba, axis=1)\n",
    "        \n",
    "    def plot_learning_curve(self, X, y, metrics='acc', params={}): \n",
    "        past_train_accuracy, past_test_accuracy, past_train_costs, past_test_costs = self.train(X, y, params)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        # count_epoch = self.params['iteration'] // self.params['data_size'] + 1\n",
    "        if metrics == 'cost':\n",
    "            plt.plot(past_train_costs, color='orange', label='train')\n",
    "            plt.plot(past_test_costs, color='lime', label='test')\n",
    "            plt.ylabel(\"cost\", fontsize=15)\n",
    "            print(\"last train cost is {}\".format(past_train_costs[-1]))\n",
    "            print(\"last test cost is {}\".format(past_test_costs[-1]))\n",
    "        else:\n",
    "            #plt.plot(np.array(past_train_accuracy), color='r')\n",
    "            plt.plot(past_train_accuracy, color='orange', label='train')\n",
    "            plt.plot(past_test_accuracy, color='lime', label='test')\n",
    "            plt.ylabel(\"accuracy\", fontsize=15)\n",
    "            print(\"last train accuracy is {}\".format(past_train_accuracy[-1]))\n",
    "            print(\"last test accuracy is {}\".format(past_test_accuracy[-1]))\n",
    "            plt.ylim(-0.5, 100.5)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title('Learning Curve', fontsize=20)\n",
    "        plt.xlabel(\"iteration[epoch]\", fontsize=15)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    " model = DNN(iteration=10, optimizer='adam', \n",
    "         hidden_layer_list = [100], batch_norm=True)\n",
    "\n",
    "params = {'optimizer': 'adam',\n",
    "#          'batch_mode' : 'mini',\n",
    "#          'init': 'he',\n",
    "         'lr': 0.01}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_digit = x_train[:, :, :, np.newaxis]\n",
    "# y_digit = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_cifar10.reshape(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before layer making (80, 32, 32, 3)\n",
      "(80, 32, 32, 3)\n",
      "#################################\n",
      " Layer 0\n",
      "Conv : out (80, 32, 32, 6) filter 5 stride 1 pad 2 \n",
      "#################################\n",
      " Layer 1\n",
      "Activation : out (80, 32, 32, 6)   func : relu \n",
      "#################################\n",
      " Layer 2\n",
      "Pooling : out (80, 16, 16, 6) filter 2 stride 2 pad 0 \n",
      "#################################\n",
      " Layer 3\n",
      "Conv : out (80, 16, 16, 16) filter 5 stride 1 pad 2 \n",
      "#################################\n",
      " Layer 4\n",
      "Activation : out (80, 16, 16, 16)   func : relu \n",
      "#################################\n",
      " Layer 5\n",
      "Pooling : out (80, 8, 8, 16) filter 2 stride 2 pad 0 \n",
      "#################################\n",
      " Layer 6\n",
      "Flatten : out (80, 1024) \n",
      "#################################\n",
      " Layer 7\n",
      "Affine : out (80, 120) optimizer adam unit 120 \n",
      "#################################\n",
      " Layer 8\n",
      "Activation : out (80, 120)   func : relu \n",
      "#################################\n",
      " Layer 9\n",
      "Affine : out (80, 84) optimizer adam unit 84 \n",
      "#################################\n",
      " Layer 10\n",
      "Activation : out (80, 84)   func : relu \n",
      "#################################\n",
      " Layer 11\n",
      "Affine : out (80, 10) optimizer adam unit 10 \n",
      "#################################\n",
      "x.shape (8, 32, 32, 3)\n",
      "x_2dim.shape (24576, 25)\n",
      "filter_size 5 stride 1 pad 2\n",
      "in_C 3 out_h 32 out_w 32 N 80 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 614400 into shape (3,newaxis,81920)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-890a77408c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_cifar10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-271-98fad39ba62b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, params)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# 誤差逆伝播法によって勾配を求め、値を更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# 1エポックごとに正答率とコストを算出して保存する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-270-a8643fcdd0a3>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-270-a8643fcdd0a3>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-270-a8643fcdd0a3>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# argmaxでラベルを取れる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-265-17778892e344>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mW_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"in_C {} out_h {} out_w {} N {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mx_2dim_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_2dim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_w\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# 入力チャネルごとに対応するフィルターで計算する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 614400 into shape (3,newaxis,81920)"
     ]
    }
   ],
   "source": [
    "pred_y = model.train(x_train_cifar10, y_train_cifar10.reshape(100,10), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
